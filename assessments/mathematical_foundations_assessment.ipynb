{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mathematical Foundations Assessment\n",
        "\n",
        "> **\"Mathematics is the language in which God has written the universe.\"** - Galileo Galilei\n",
        "\n",
        "## Assessment Overview\n",
        "\n",
        "This comprehensive assessment evaluates your mastery of mathematical foundations essential for machine learning. The questions are designed to test not just computational skills, but deep conceptual understanding and the ability to connect mathematical theory to practical applications.\n",
        "\n",
        "### Assessment Structure\n",
        "- **15 Questions** covering Linear Algebra, Probability, Statistics, and Optimization\n",
        "- **Difficulty Levels**: Foundational (1-5), Intermediate (6-10), Advanced (11-15)\n",
        "- **Time Limit**: 2 hours\n",
        "- **Format**: Mix of theoretical, computational, and applied problems\n",
        "\n",
        "### Learning Objectives Tested\n",
        "- Vector operations and geometric interpretations\n",
        "- Matrix decompositions and their applications\n",
        "- Probability theory and Bayesian inference\n",
        "- Statistical distributions and hypothesis testing\n",
        "- Optimization theory and gradient methods\n",
        "- Information theory and entropy\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. **Read each question carefully** - Many questions have multiple parts\n",
        "2. **Show your work** - Partial credit will be given for correct methodology\n",
        "3. **Explain your reasoning** - Understanding the \"why\" is as important as the \"how\"\n",
        "4. **Use appropriate notation** - Mathematical rigor is expected\n",
        "5. **Connect to ML applications** - Where relevant, explain how concepts apply to machine learning\n",
        "\n",
        "**Good luck!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1: Vector Geometry and Machine Learning (Foundational)\n",
        "\n",
        "Consider the vectors **a** = [3, 4, 0] and **b** = [1, 2, 2] in ℝ³.\n",
        "\n",
        "**a)** Calculate the dot product **a · b** and explain what this value represents geometrically.\n",
        "\n",
        "**b)** Find the angle between vectors **a** and **b**.\n",
        "\n",
        "**c)** In the context of machine learning, explain how the dot product is used in:\n",
        "   - Linear regression\n",
        "   - Neural network computations\n",
        "   - Similarity measures between data points\n",
        "\n",
        "**d)** Calculate the projection of vector **a** onto vector **b** and interpret this result.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2: Matrix Operations and Linear Transformations (Foundational)\n",
        "\n",
        "Given the matrix **A** = [[2, 1], [1, 3]]:\n",
        "\n",
        "**a)** Calculate the determinant of **A** and explain its geometric significance.\n",
        "\n",
        "**b)** Find the eigenvalues and eigenvectors of **A**.\n",
        "\n",
        "**c)** Perform the eigendecomposition **A = QΛQᵀ** and verify your result.\n",
        "\n",
        "**d)** Explain how eigendecomposition is used in:\n",
        "   - Principal Component Analysis (PCA)\n",
        "   - Linear regression with multiple features\n",
        "   - Dimensionality reduction techniques\n",
        "\n",
        "---\n",
        "\n",
        "## Question 3: Probability Theory and Bayesian Inference (Foundational)\n",
        "\n",
        "A medical test for a disease has:\n",
        "- Sensitivity (True Positive Rate): 95%\n",
        "- Specificity (True Negative Rate): 90%\n",
        "- Disease prevalence in population: 2%\n",
        "\n",
        "**a)** Calculate the probability that a person has the disease given a positive test result.\n",
        "\n",
        "**b)** Calculate the probability that a person does not have the disease given a negative test result.\n",
        "\n",
        "**c)** Explain why the probability in part (a) is much lower than the test's sensitivity.\n",
        "\n",
        "**d)** In machine learning, explain how this concept applies to:\n",
        "   - Class imbalance problems\n",
        "   - Precision and recall metrics\n",
        "   - Calibrating classifier outputs\n",
        "\n",
        "---\n",
        "\n",
        "## Question 4: Statistical Distributions and Hypothesis Testing (Foundational)\n",
        "\n",
        "A machine learning model is trained to predict house prices. The residuals (actual - predicted) are normally distributed with mean 0 and standard deviation $15,000.\n",
        "\n",
        "**a)** What is the probability that a prediction error exceeds $30,000?\n",
        "\n",
        "**b)** What is the probability that a prediction error is between -$10,000 and $20,000?\n",
        "\n",
        "**c)** If we want 95% of predictions to be within a certain range, what should that range be?\n",
        "\n",
        "**d)** Explain how the Central Limit Theorem applies to:\n",
        "   - Cross-validation estimates\n",
        "   - Bootstrap sampling\n",
        "   - Confidence intervals for model performance\n",
        "\n",
        "---\n",
        "\n",
        "## Question 5: Optimization Theory and Gradient Descent (Foundational)\n",
        "\n",
        "Consider the function f(x, y) = x² + 2y² - 4x - 8y + 20.\n",
        "\n",
        "**a)** Find the gradient ∇f(x, y) and the critical point(s).\n",
        "\n",
        "**b)** Determine whether the critical point is a minimum, maximum, or saddle point using the second derivative test.\n",
        "\n",
        "**c)** Starting from point (0, 0), perform two iterations of gradient descent with learning rate α = 0.1.\n",
        "\n",
        "**d)** Explain how gradient descent is used in:\n",
        "   - Linear regression\n",
        "   - Logistic regression\n",
        "   - Neural network training\n",
        "   - What are the advantages and disadvantages compared to analytical solutions?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 6: Linear Algebra in Dimensionality Reduction (Intermediate)\n",
        "\n",
        "Given a dataset X with n=100 samples and p=5 features, where X is centered (mean=0).\n",
        "\n",
        "**a)** Derive the formula for the covariance matrix C = (1/n)XᵀX.\n",
        "\n",
        "**b)** Explain why the covariance matrix is symmetric and positive semi-definite.\n",
        "\n",
        "**c)** If the eigenvalues of C are [10, 5, 2, 0.5, 0.1], calculate the proportion of variance explained by the first two principal components.\n",
        "\n",
        "**d)** In the context of PCA, explain:\n",
        "   - Why we center the data before computing the covariance matrix\n",
        "   - The relationship between eigenvalues and variance\n",
        "   - How to choose the number of components to retain\n",
        "\n",
        "---\n",
        "\n",
        "## Question 7: Information Theory and Entropy (Intermediate)\n",
        "\n",
        "Consider a binary classification problem with the following class distribution:\n",
        "- Class 0: 70% of samples\n",
        "- Class 1: 30% of samples\n",
        "\n",
        "**a)** Calculate the entropy H(Y) of the class distribution.\n",
        "\n",
        "**b)** If a feature X splits the data such that:\n",
        "   - When X=0: 60% Class 0, 40% Class 1\n",
        "   - When X=1: 80% Class 0, 20% Class 1\n",
        "   \n",
        "   Calculate the conditional entropy H(Y|X) and information gain IG(Y,X).\n",
        "\n",
        "**c)** Explain how information gain is used in decision tree construction.\n",
        "\n",
        "**d)** In machine learning, explain the relationship between:\n",
        "   - Entropy and uncertainty\n",
        "   - Information gain and feature selection\n",
        "   - Mutual information and feature relevance\n",
        "\n",
        "---\n",
        "\n",
        "## Question 8: Advanced Probability and Maximum Likelihood (Intermediate)\n",
        "\n",
        "A machine learning model assumes that data points x₁, x₂, ..., xₙ are independently drawn from a normal distribution N(μ, σ²).\n",
        "\n",
        "**a)** Write the likelihood function L(μ, σ²) for the observed data.\n",
        "\n",
        "**b)** Derive the maximum likelihood estimators for μ and σ².\n",
        "\n",
        "**c)** Show that the MLE for μ is unbiased, but the MLE for σ² is biased.\n",
        "\n",
        "**d)** Explain how maximum likelihood estimation is used in:\n",
        "   - Logistic regression\n",
        "   - Gaussian Naive Bayes\n",
        "   - Neural network parameter estimation\n",
        "\n",
        "---\n",
        "\n",
        "## Question 9: Matrix Decompositions and Numerical Stability (Intermediate)\n",
        "\n",
        "Consider the matrix **A** = [[4, 2], [2, 3]].\n",
        "\n",
        "**a)** Perform Cholesky decomposition of **A**.\n",
        "\n",
        "**b)** Perform QR decomposition of **A**.\n",
        "\n",
        "**c)** Compare the computational complexity of solving **Ax = b** using:\n",
        "   - Gaussian elimination\n",
        "   - Cholesky decomposition\n",
        "   - QR decomposition\n",
        "\n",
        "**d)** In machine learning, explain when you would use each decomposition:\n",
        "   - Cholesky: Linear regression with normal equations\n",
        "   - QR: Least squares problems\n",
        "   - SVD: Dimensionality reduction and regularization\n",
        "\n",
        "---\n",
        "\n",
        "## Question 10: Advanced Optimization and Convexity (Intermediate)\n",
        "\n",
        "Consider the function f(x) = x₁² + x₂² + 2x₁x₂ + 4x₁ + 6x₂ + 10.\n",
        "\n",
        "**a)** Write f(x) in the form f(x) = (1/2)xᵀQx + cᵀx + d.\n",
        "\n",
        "**b)** Determine if f(x) is convex by examining the Hessian matrix.\n",
        "\n",
        "**c)** Find the global minimum using the analytical solution.\n",
        "\n",
        "**d)** Explain the importance of convexity in machine learning:\n",
        "   - Why do we prefer convex optimization problems?\n",
        "   - What happens when the objective function is non-convex?\n",
        "   - How does regularization affect convexity?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 11: Advanced Linear Algebra and Kernel Methods (Advanced)\n",
        "\n",
        "Given a dataset with n samples, consider the kernel matrix **K** where Kᵢⱼ = k(xᵢ, xⱼ) for some kernel function k.\n",
        "\n",
        "**a)** Prove that any kernel matrix is positive semi-definite.\n",
        "\n",
        "**b)** For the RBF kernel k(x, y) = exp(-γ||x-y||²), explain why it's called the \"Gaussian\" kernel.\n",
        "\n",
        "**c)** In Support Vector Machines, explain how the kernel trick allows us to work in high-dimensional feature spaces without explicitly computing the features.\n",
        "\n",
        "**d)** Derive the dual optimization problem for SVM with kernel k(x, y).\n",
        "\n",
        "---\n",
        "\n",
        "## Question 12: Advanced Probability and Bayesian Methods (Advanced)\n",
        "\n",
        "In Bayesian linear regression, we assume:\n",
        "- Likelihood: y|X, w, σ² ~ N(Xw, σ²I)\n",
        "- Prior: w ~ N(0, α⁻¹I)\n",
        "\n",
        "**a)** Derive the posterior distribution p(w|y, X, σ², α).\n",
        "\n",
        "**b)** Show that the posterior mean is equivalent to Ridge regression with λ = σ²/α.\n",
        "\n",
        "**c)** Derive the predictive distribution p(y*|x*, y, X, σ², α) for a new input x*.\n",
        "\n",
        "**d)** Explain the advantages of Bayesian methods:\n",
        "   - Uncertainty quantification\n",
        "   - Automatic regularization\n",
        "   - Model selection through marginal likelihood\n",
        "\n",
        "---\n",
        "\n",
        "## Question 13: Advanced Statistics and Model Selection (Advanced)\n",
        "\n",
        "Consider model selection using AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion).\n",
        "\n",
        "**a)** Derive the AIC formula: AIC = 2k - 2ln(L), where k is the number of parameters and L is the likelihood.\n",
        "\n",
        "**b)** Derive the BIC formula: BIC = k·ln(n) - 2ln(L), where n is the sample size.\n",
        "\n",
        "**c)** Compare AIC and BIC in terms of:\n",
        "   - Model complexity penalty\n",
        "   - Asymptotic behavior\n",
        "   - When to use each criterion\n",
        "\n",
        "**d)** In the context of machine learning, explain how these criteria relate to:\n",
        "   - Overfitting and underfitting\n",
        "   - Cross-validation\n",
        "   - Regularization methods\n",
        "\n",
        "---\n",
        "\n",
        "## Question 14: Advanced Optimization and Stochastic Methods (Advanced)\n",
        "\n",
        "Consider stochastic gradient descent (SGD) for minimizing f(w) = (1/n)Σᵢ₌₁ⁿ fᵢ(w).\n",
        "\n",
        "**a)** Derive the SGD update rule: wₜ₊₁ = wₜ - αₜ∇fᵢₜ(wₜ), where iₜ is randomly sampled.\n",
        "\n",
        "**b)** Show that E[∇fᵢₜ(wₜ)] = ∇f(wₜ), i.e., SGD is an unbiased estimator of the true gradient.\n",
        "\n",
        "**c)** Explain the convergence conditions for SGD:\n",
        "   - Learning rate schedule\n",
        "   - Gradient variance bounds\n",
        "   - Strong convexity requirements\n",
        "\n",
        "**d)** Compare SGD with batch gradient descent in terms of:\n",
        "   - Computational complexity per iteration\n",
        "   - Convergence rate\n",
        "   - Memory requirements\n",
        "   - Practical considerations\n",
        "\n",
        "---\n",
        "\n",
        "## Question 15: Integration and Advanced Applications (Advanced)\n",
        "\n",
        "Consider a machine learning pipeline that processes high-dimensional data through multiple stages:\n",
        "1. Data preprocessing and normalization\n",
        "2. Dimensionality reduction using PCA\n",
        "3. Feature selection using mutual information\n",
        "4. Model training with regularization\n",
        "5. Model evaluation and validation\n",
        "\n",
        "**a)** For each stage, identify the key mathematical concepts involved and explain their role.\n",
        "\n",
        "**b)** Design a mathematical framework for:\n",
        "   - Combining multiple models (ensemble methods)\n",
        "   - Handling missing data probabilistically\n",
        "   - Quantifying prediction uncertainty\n",
        "\n",
        "**c)** Explain how the mathematical foundations you've learned enable:\n",
        "   - Scalable algorithms for big data\n",
        "   - Robust models that generalize well\n",
        "   - Interpretable machine learning systems\n",
        "\n",
        "**d)** Propose a research direction that builds upon these mathematical foundations to address current limitations in machine learning.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Answer Key and Solutions\n",
        "\n",
        "### Question 1 Solutions\n",
        "\n",
        "**a)** Dot product: **a · b** = 3×1 + 4×2 + 0×2 = 3 + 8 + 0 = 11\n",
        "\n",
        "Geometrically, this represents the product of the magnitudes of the vectors and the cosine of the angle between them.\n",
        "\n",
        "**b)** Angle calculation:\n",
        "||**a**|| = √(3² + 4² + 0²) = 5\n",
        "||**b**|| = √(1² + 2² + 2²) = 3\n",
        "cos(θ) = (**a · b**)/(||**a**|| ||**b**||) = 11/(5×3) = 11/15\n",
        "θ = arccos(11/15) ≈ 42.8°\n",
        "\n",
        "**c)** ML applications:\n",
        "- Linear regression: y = wᵀx + b (dot product between weights and features)\n",
        "- Neural networks: Each neuron computes a weighted sum (dot product)\n",
        "- Similarity: Cosine similarity uses normalized dot product\n",
        "\n",
        "**d)** Projection: proj_b(**a**) = ((**a · b**)/||**b**||²)**b** = (11/9)[1, 2, 2] = [11/9, 22/9, 22/9]\n",
        "\n",
        "This represents how much of vector **a** points in the direction of vector **b**.\n",
        "\n",
        "---\n",
        "\n",
        "### Question 2 Solutions\n",
        "\n",
        "**a)** Determinant: det(**A**) = 2×3 - 1×1 = 6 - 1 = 5\n",
        "\n",
        "Geometrically, this represents the area scaling factor of the linear transformation.\n",
        "\n",
        "**b)** Eigenvalues and eigenvectors:\n",
        "Characteristic equation: det(**A** - λ**I**) = 0\n",
        "(2-λ)(3-λ) - 1 = 0\n",
        "λ² - 5λ + 5 = 0\n",
        "λ = (5 ± √5)/2\n",
        "\n",
        "For λ₁ = (5 + √5)/2: eigenvector [1, (1+√5)/2]\n",
        "For λ₂ = (5 - √5)/2: eigenvector [1, (1-√5)/2]\n",
        "\n",
        "**c)** Eigendecomposition verification:\n",
        "**Q** = [[1, 1], [(1+√5)/2, (1-√5)/2]]\n",
        "**Λ** = [[(5+√5)/2, 0], [0, (5-√5)/2]]\n",
        "**A** = **QΛQᵀ** (verification by matrix multiplication)\n",
        "\n",
        "**d)** Applications:\n",
        "- PCA: Eigenvectors of covariance matrix are principal components\n",
        "- Linear regression: Normal equations involve matrix inverses\n",
        "- Dimensionality reduction: Project data onto subspace spanned by top eigenvectors\n",
        "\n",
        "---\n",
        "\n",
        "### Question 3 Solutions\n",
        "\n",
        "**a)** Using Bayes' theorem:\n",
        "P(Disease|Positive) = P(Positive|Disease) × P(Disease) / P(Positive)\n",
        "P(Positive) = P(Positive|Disease) × P(Disease) + P(Positive|No Disease) × P(No Disease)\n",
        "P(Positive) = 0.95 × 0.02 + 0.10 × 0.98 = 0.019 + 0.098 = 0.117\n",
        "P(Disease|Positive) = 0.95 × 0.02 / 0.117 ≈ 0.162 (16.2%)\n",
        "\n",
        "**b)** P(No Disease|Negative) = P(Negative|No Disease) × P(No Disease) / P(Negative)\n",
        "P(Negative) = 0.90 × 0.98 + 0.05 × 0.02 = 0.882 + 0.001 = 0.883\n",
        "P(No Disease|Negative) = 0.90 × 0.98 / 0.883 ≈ 0.999 (99.9%)\n",
        "\n",
        "**c)** The probability is low because the disease is rare (2% prevalence). Even with a good test, most positive results are false positives due to the large number of healthy people.\n",
        "\n",
        "**d)** ML applications:\n",
        "- Class imbalance: Rare classes need special handling\n",
        "- Precision/Recall: Trade-off between false positives and false negatives\n",
        "- Calibration: Adjusting classifier outputs to reflect true probabilities\n",
        "\n",
        "---\n",
        "\n",
        "### Question 4 Solutions\n",
        "\n",
        "**a)** P(|error| > 30,000) = 2 × P(error > 30,000) = 2 × P(Z > 2) = 2 × 0.0228 = 0.0456\n",
        "\n",
        "**b)** P(-10,000 < error < 20,000) = P(-0.67 < Z < 1.33) = 0.9082 - 0.2514 = 0.6568\n",
        "\n",
        "**c)** For 95% confidence: P(-1.96 < Z < 1.96) = 0.95\n",
        "Range: ±1.96 × 15,000 = ±29,400\n",
        "\n",
        "**d)** CLT applications:\n",
        "- Cross-validation: Sample means of performance metrics are approximately normal\n",
        "- Bootstrap: Distribution of bootstrap statistics approaches normal\n",
        "- Confidence intervals: Based on normal approximation of sampling distributions\n",
        "\n",
        "---\n",
        "\n",
        "### Question 5 Solutions\n",
        "\n",
        "**a)** ∇f = [2x - 4, 4y - 8]\n",
        "Critical point: 2x - 4 = 0 → x = 2, 4y - 8 = 0 → y = 2\n",
        "Critical point: (2, 2)\n",
        "\n",
        "**b)** Hessian matrix: H = [[2, 0], [0, 4]]\n",
        "Eigenvalues: 2, 4 (both positive) → local minimum\n",
        "\n",
        "**c)** Gradient descent iterations:\n",
        "Iteration 1: ∇f(0,0) = [-4, -8], x₁ = [0,0] - 0.1[-4,-8] = [0.4, 0.8]\n",
        "Iteration 2: ∇f(0.4,0.8) = [-3.2, -4.8], x₂ = [0.4,0.8] - 0.1[-3.2,-4.8] = [0.72, 1.28]\n",
        "\n",
        "**d)** Gradient descent applications:\n",
        "- Linear regression: Minimize MSE\n",
        "- Logistic regression: Minimize cross-entropy\n",
        "- Neural networks: Backpropagation\n",
        "- Advantages: Works for large datasets, handles non-analytical solutions\n",
        "- Disadvantages: Requires tuning learning rate, may converge slowly\n",
        "\n",
        "---\n",
        "\n",
        "## Grading Rubric\n",
        "\n",
        "### Excellent (90-100%)\n",
        "- Correct mathematical derivations\n",
        "- Clear explanations of concepts\n",
        "- Strong connections to ML applications\n",
        "- Demonstrates deep understanding\n",
        "\n",
        "### Good (80-89%)\n",
        "- Mostly correct solutions\n",
        "- Good understanding of concepts\n",
        "- Some connections to applications\n",
        "- Minor errors in derivations\n",
        "\n",
        "### Satisfactory (70-79%)\n",
        "- Basic understanding shown\n",
        "- Some correct solutions\n",
        "- Limited connections to applications\n",
        "- Several computational errors\n",
        "\n",
        "### Needs Improvement (60-69%)\n",
        "- Limited understanding\n",
        "- Many computational errors\n",
        "- Weak connections to applications\n",
        "- Incomplete solutions\n",
        "\n",
        "### Unsatisfactory (<60%)\n",
        "- Little understanding demonstrated\n",
        "- Major errors throughout\n",
        "- No connections to applications\n",
        "- Incomplete or missing solutions\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations on completing the Mathematical Foundations Assessment!**\n",
        "\n",
        "This assessment tests the fundamental mathematical knowledge required for advanced machine learning. Mastery of these concepts will enable you to understand, implement, and innovate in machine learning algorithms and applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
