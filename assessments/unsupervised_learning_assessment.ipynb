{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unsupervised Learning Assessment\n",
        "\n",
        "> **\"The goal of unsupervised learning is to find hidden patterns in data without labeled examples.\"** - Yann LeCun\n",
        "\n",
        "## Assessment Overview\n",
        "\n",
        "This comprehensive assessment evaluates your mastery of unsupervised learning algorithms, their mathematical foundations, and practical applications. The questions test both theoretical understanding and practical implementation skills.\n",
        "\n",
        "### Assessment Structure\n",
        "- **15 Questions** covering Clustering, Dimensionality Reduction, Association Rules, and Anomaly Detection\n",
        "- **Difficulty Levels**: Foundational (1-5), Intermediate (6-10), Advanced (11-15)\n",
        "- **Time Limit**: 2.5 hours\n",
        "- **Format**: Mix of theoretical, computational, and implementation problems\n",
        "\n",
        "### Learning Objectives Tested\n",
        "- Clustering algorithms and distance metrics\n",
        "- Dimensionality reduction techniques\n",
        "- Association rule mining and market basket analysis\n",
        "- Anomaly detection and outlier identification\n",
        "- Feature extraction and representation learning\n",
        "- Model evaluation without ground truth\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. **Read each question carefully** - Many questions have multiple parts\n",
        "2. **Show your work** - Partial credit will be given for correct methodology\n",
        "3. **Explain your reasoning** - Understanding the \"why\" is as important as the \"how\"\n",
        "4. **Use appropriate notation** - Mathematical rigor is expected\n",
        "5. **Connect to real-world applications** - Where relevant, explain practical implications\n",
        "\n",
        "**Good luck!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1: K-Means Clustering Fundamentals (Foundational)\n",
        "\n",
        "Consider a dataset with 100 points in 2D space that you want to cluster into k=3 groups.\n",
        "\n",
        "**a)** Derive the K-means objective function and explain what it minimizes.\n",
        "\n",
        "**b)** Show that the optimal centroid for each cluster is the mean of the points assigned to that cluster.\n",
        "\n",
        "**c)** Implement the K-means algorithm step-by-step for the first two iterations, starting with random centroids.\n",
        "\n",
        "**d)** Explain the limitations of K-means and when it might fail:\n",
        "   - Non-spherical clusters\n",
        "   - Different cluster sizes\n",
        "   - Outliers\n",
        "   - High-dimensional data\n",
        "\n",
        "---\n",
        "\n",
        "## Question 2: Hierarchical Clustering (Foundational)\n",
        "\n",
        "You have a dataset with 5 points: A(1,1), B(1,2), C(4,4), D(5,5), E(5,6).\n",
        "\n",
        "**a)** Perform single-linkage hierarchical clustering and draw the dendrogram.\n",
        "\n",
        "**b)** Perform complete-linkage hierarchical clustering and compare with single-linkage.\n",
        "\n",
        "**c)** Calculate the cophenetic correlation coefficient for both methods.\n",
        "\n",
        "**d)** Compare hierarchical clustering with K-means:\n",
        "   - Computational complexity\n",
        "   - Deterministic vs. stochastic\n",
        "   - Handling of different cluster shapes\n",
        "   - When to use each method\n",
        "\n",
        "---\n",
        "\n",
        "## Question 3: Principal Component Analysis (Foundational)\n",
        "\n",
        "Given a dataset X with n=100 samples and p=3 features, where the covariance matrix is:\n",
        "C = [[4, 2, 1], [2, 3, 0.5], [1, 0.5, 2]]\n",
        "\n",
        "**a)** Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
        "\n",
        "**b)** Determine how many principal components to retain if you want to explain 90% of the variance.\n",
        "\n",
        "**c)** Derive the first principal component and explain its geometric interpretation.\n",
        "\n",
        "**d)** Explain the assumptions and limitations of PCA:\n",
        "   - Linear relationships\n",
        "   - Gaussian distribution\n",
        "   - Scale sensitivity\n",
        "   - Interpretability\n",
        "\n",
        "---\n",
        "\n",
        "## Question 4: Association Rule Mining (Foundational)\n",
        "\n",
        "Consider a market basket dataset with the following transactions:\n",
        "- T1: {bread, milk, butter}\n",
        "- T2: {bread, milk, eggs}\n",
        "- T3: {milk, eggs, cheese}\n",
        "- T4: {bread, milk, eggs, cheese}\n",
        "- T5: {bread, eggs}\n",
        "\n",
        "**a)** Calculate support, confidence, and lift for the rule {bread, milk} → {eggs}.\n",
        "\n",
        "**b)** Find all frequent itemsets with minimum support = 0.4.\n",
        "\n",
        "**c)** Generate all association rules with minimum confidence = 0.6 and minimum lift = 1.2.\n",
        "\n",
        "**d)** Explain how to handle:\n",
        "   - Large datasets efficiently\n",
        "   - Rare items\n",
        "   - Multiple minimum thresholds\n",
        "   - Rule pruning strategies\n",
        "\n",
        "---\n",
        "\n",
        "## Question 5: Anomaly Detection (Foundational)\n",
        "\n",
        "You have a dataset of server response times (in milliseconds) with the following statistics:\n",
        "- Mean: 200ms\n",
        "- Standard deviation: 50ms\n",
        "- Sample size: 10,000\n",
        "\n",
        "**a)** Using the 3-sigma rule, identify potential anomalies and calculate the expected number of false positives.\n",
        "\n",
        "**b)** Implement a simple anomaly detection algorithm using the Z-score method.\n",
        "\n",
        "**c)** Compare statistical methods with machine learning approaches for anomaly detection:\n",
        "   - Isolation Forest\n",
        "   - One-Class SVM\n",
        "   - Local Outlier Factor (LOF)\n",
        "\n",
        "**d)** Design an anomaly detection system that:\n",
        "   - Handles different types of anomalies\n",
        "   - Adapts to changing data distributions\n",
        "   - Provides confidence scores\n",
        "   - Minimizes false positives\n",
        "\n",
        "---\n",
        "\n",
        "## Question 6: Advanced Clustering Algorithms (Intermediate)\n",
        "\n",
        "Consider a dataset with clusters of different shapes, sizes, and densities.\n",
        "\n",
        "**a)** Explain how DBSCAN works and derive its time complexity.\n",
        "\n",
        "**b)** Compare DBSCAN with K-means in terms of:\n",
        "   - Number of clusters (known vs. unknown)\n",
        "   - Cluster shapes (spherical vs. arbitrary)\n",
        "   - Noise handling\n",
        "   - Parameter sensitivity\n",
        "\n",
        "**c)** Design a clustering algorithm that:\n",
        "   - Automatically determines the number of clusters\n",
        "   - Handles clusters of different densities\n",
        "   - Is robust to outliers\n",
        "   - Works well in high dimensions\n",
        "\n",
        "**d)** Implement a clustering validation framework that:\n",
        "   - Uses multiple internal validation metrics\n",
        "   - Handles different cluster shapes\n",
        "   - Provides confidence intervals\n",
        "   - Compares different algorithms\n",
        "\n",
        "---\n",
        "\n",
        "## Question 7: Non-linear Dimensionality Reduction (Intermediate)\n",
        "\n",
        "You have a dataset that lies on a non-linear manifold (e.g., Swiss roll, S-curve).\n",
        "\n",
        "**a)** Explain why PCA fails for non-linear manifolds and derive the first principal component.\n",
        "\n",
        "**b)** Implement t-SNE algorithm step-by-step for 2D visualization.\n",
        "\n",
        "**c)** Compare different non-linear dimensionality reduction methods:\n",
        "   - t-SNE\n",
        "   - UMAP\n",
        "   - Isomap\n",
        "   - Locally Linear Embedding (LLE)\n",
        "\n",
        "**d)** Design a dimensionality reduction pipeline that:\n",
        "   - Automatically chooses between linear and non-linear methods\n",
        "   - Preserves local and global structure\n",
        "   - Handles different data types\n",
        "   - Provides quality metrics\n",
        "\n",
        "---\n",
        "\n",
        "## Question 8: Advanced Association Rule Mining (Intermediate)\n",
        "\n",
        "Consider a large e-commerce dataset with millions of transactions and thousands of items.\n",
        "\n",
        "**a)** Implement the Apriori algorithm efficiently and explain its pruning strategy.\n",
        "\n",
        "**b)** Design a parallel version of Apriori that can handle big data.\n",
        "\n",
        "**c)** Extend association rule mining to handle:\n",
        "   - Temporal patterns\n",
        "   - Weighted items\n",
        "   - Hierarchical categories\n",
        "   - Negative associations\n",
        "\n",
        "**d)** Create a recommendation system based on association rules that:\n",
        "   - Handles cold start problems\n",
        "   - Incorporates user preferences\n",
        "   - Updates in real-time\n",
        "   - Provides explanations\n",
        "\n",
        "---\n",
        "\n",
        "## Question 9: Advanced Anomaly Detection (Intermediate)\n",
        "\n",
        "You need to detect anomalies in a streaming data system with the following challenges:\n",
        "- Concept drift\n",
        "- High-dimensional data\n",
        "- Multiple data types\n",
        "- Real-time requirements\n",
        "\n",
        "**a)** Design an online anomaly detection algorithm that adapts to concept drift.\n",
        "\n",
        "**b)** Implement a multi-modal anomaly detection system for mixed data types.\n",
        "\n",
        "**c)** Compare different approaches for high-dimensional anomaly detection:\n",
        "   - Isolation Forest\n",
        "   - One-Class SVM with RBF kernel\n",
        "   - Autoencoders\n",
        "   - Local Outlier Factor\n",
        "\n",
        "**d)** Create an anomaly detection system that:\n",
        "   - Provides different types of explanations\n",
        "   - Handles false positive feedback\n",
        "   - Scales to millions of data points\n",
        "   - Integrates with alerting systems\n",
        "\n",
        "---\n",
        "\n",
        "## Question 10: Clustering Validation and Evaluation (Intermediate)\n",
        "\n",
        "You need to evaluate clustering results without ground truth labels.\n",
        "\n",
        "**a)** Derive the Silhouette coefficient and explain its interpretation.\n",
        "\n",
        "**b)** Implement multiple internal validation metrics:\n",
        "   - Calinski-Harabasz index\n",
        "   - Davies-Bouldin index\n",
        "   - Gap statistic\n",
        "\n",
        "**c)** Design a comprehensive clustering evaluation framework that:\n",
        "   - Combines multiple metrics\n",
        "   - Handles different cluster shapes\n",
        "   - Provides statistical significance tests\n",
        "   - Accounts for computational constraints\n",
        "\n",
        "**d)** Compare different approaches for determining the optimal number of clusters:\n",
        "   - Elbow method\n",
        "   - Gap statistic\n",
        "   - Information criteria\n",
        "   - Stability analysis\n",
        "\n",
        "---\n",
        "\n",
        "## Question 11: Advanced Dimensionality Reduction (Advanced)\n",
        "\n",
        "You have a high-dimensional dataset with complex structure that requires sophisticated dimensionality reduction.\n",
        "\n",
        "**a)** Implement a variational autoencoder (VAE) for dimensionality reduction and explain how it differs from PCA.\n",
        "\n",
        "**b)** Design a manifold learning algorithm that:\n",
        "   - Preserves both local and global structure\n",
        "   - Handles noise and outliers\n",
        "   - Works with different distance metrics\n",
        "   - Provides uncertainty estimates\n",
        "\n",
        "**c)** Compare different approaches for handling high-dimensional data:\n",
        "   - Random projections\n",
        "   - Sparse PCA\n",
        "   - Non-negative matrix factorization\n",
        "   - Independent Component Analysis (ICA)\n",
        "\n",
        "**d)** Create a dimensionality reduction system that:\n",
        "   - Automatically selects the optimal method\n",
        "   - Handles different data types\n",
        "   - Provides interpretable results\n",
        "   - Scales to very large datasets\n",
        "\n",
        "---\n",
        "\n",
        "## Question 12: Advanced Clustering Algorithms (Advanced)\n",
        "\n",
        "You need to cluster a complex dataset with overlapping clusters, noise, and varying densities.\n",
        "\n",
        "**a)** Implement a Gaussian Mixture Model (GMM) with EM algorithm and derive the update equations.\n",
        "\n",
        "**b)** Design a hierarchical clustering algorithm that:\n",
        "   - Uses different distance metrics\n",
        "   - Handles overlapping clusters\n",
        "   - Provides uncertainty estimates\n",
        "   - Scales to large datasets\n",
        "\n",
        "**c)** Compare different clustering paradigms:\n",
        "   - Partitioning (K-means, K-medoids)\n",
        "   - Hierarchical (Agglomerative, Divisive)\n",
        "   - Density-based (DBSCAN, OPTICS)\n",
        "   - Model-based (GMM, Bayesian)\n",
        "\n",
        "**d)** Create a clustering system that:\n",
        "   - Automatically determines the number of clusters\n",
        "   - Handles different cluster shapes and sizes\n",
        "   - Provides cluster descriptions\n",
        "   - Integrates with downstream tasks\n",
        "\n",
        "---\n",
        "\n",
        "## Question 13: Advanced Association Rule Mining (Advanced)\n",
        "\n",
        "You need to mine complex patterns from a large, multi-dimensional dataset.\n",
        "\n",
        "**a)** Implement FP-Growth algorithm and compare its efficiency with Apriori.\n",
        "\n",
        "**b)** Design a system for mining sequential patterns that:\n",
        "   - Handles temporal dependencies\n",
        "   - Incorporates domain knowledge\n",
        "   - Provides pattern explanations\n",
        "   - Scales to streaming data\n",
        "\n",
        "**c)** Extend association rule mining to handle:\n",
        "   - Continuous variables\n",
        "   - Time series data\n",
        "   - Graph-structured data\n",
        "   - Multi-level patterns\n",
        "\n",
        "**d)** Create a pattern mining system that:\n",
        "   - Discovers interesting patterns automatically\n",
        "   - Handles different data types\n",
        "   - Provides pattern quality metrics\n",
        "   - Integrates with business intelligence\n",
        "\n",
        "---\n",
        "\n",
        "## Question 14: Advanced Anomaly Detection (Advanced)\n",
        "\n",
        "You need to detect anomalies in a complex system with multiple data sources and requirements.\n",
        "\n",
        "**a)** Implement an ensemble anomaly detection system that combines multiple algorithms.\n",
        "\n",
        "**b)** Design a deep learning approach for anomaly detection that:\n",
        "   - Uses autoencoders\n",
        "   - Handles different data types\n",
        "   - Provides uncertainty estimates\n",
        "   - Adapts to concept drift\n",
        "\n",
        "**c)** Compare different approaches for anomaly detection:\n",
        "   - Statistical methods\n",
        "   - Machine learning methods\n",
        "   - Deep learning methods\n",
        "   - Ensemble methods\n",
        "\n",
        "**d)** Create an anomaly detection system that:\n",
        "   - Handles multiple data sources\n",
        "   - Provides different types of explanations\n",
        "   - Integrates with business processes\n",
        "   - Maintains performance over time\n",
        "\n",
        "---\n",
        "\n",
        "## Question 15: Integration and Production Systems (Advanced)\n",
        "\n",
        "Design a complete unsupervised learning system for a real-world application (e.g., customer segmentation, fraud detection, or recommendation system).\n",
        "\n",
        "**a)** Design the overall system architecture including:\n",
        "   - Data preprocessing and feature engineering\n",
        "   - Multiple clustering algorithms\n",
        "   - Dimensionality reduction\n",
        "   - Anomaly detection\n",
        "   - Pattern mining\n",
        "\n",
        "**b)** Implement a robust evaluation framework that:\n",
        "   - Uses multiple validation metrics\n",
        "   - Handles different data types\n",
        "   - Provides statistical significance tests\n",
        "   - Accounts for business requirements\n",
        "\n",
        "**c)** Design a production monitoring system that:\n",
        "   - Tracks model performance\n",
        "   - Detects concept drift\n",
        "   - Handles model updates\n",
        "   - Provides alerts and dashboards\n",
        "\n",
        "**d)** Address practical considerations:\n",
        "   - Scalability and performance\n",
        "   - Interpretability and explainability\n",
        "   - Integration with existing systems\n",
        "   - Cost optimization\n",
        "   - Regulatory compliance\n",
        "\n",
        "---\n",
        "\n",
        "## Answer Key and Solutions\n",
        "\n",
        "### Question 1 Solutions\n",
        "\n",
        "**a)** K-means objective function:\n",
        "J = Σᵢ₌₁ᵏ Σₓ∈Cᵢ ||x - μᵢ||²\n",
        "This minimizes the within-cluster sum of squares.\n",
        "\n",
        "**b)** Optimal centroid derivation:\n",
        "∂J/∂μᵢ = -2Σₓ∈Cᵢ(x - μᵢ) = 0\n",
        "Σₓ∈Cᵢ(x - μᵢ) = 0\n",
        "μᵢ = (1/|Cᵢ|)Σₓ∈Cᵢ x\n",
        "\n",
        "**c)** K-means algorithm:\n",
        "1. Initialize centroids randomly\n",
        "2. Assign each point to nearest centroid\n",
        "3. Update centroids to mean of assigned points\n",
        "4. Repeat until convergence\n",
        "\n",
        "**d)** Limitations:\n",
        "- Assumes spherical clusters\n",
        "- Sensitive to initialization\n",
        "- Requires known number of clusters\n",
        "- Struggles with different cluster sizes\n",
        "- Not suitable for non-convex clusters\n",
        "\n",
        "---\n",
        "\n",
        "### Question 2 Solutions\n",
        "\n",
        "**a)** Single-linkage clustering:\n",
        "Distances: AB=1, AC=4.24, AD=5.66, AE=6.40, BC=3.16, BD=5.66, BE=6.40, CD=1.41, CE=2.24, DE=1\n",
        "Merge order: AB, CD, DE, (AB)CE, (ABCE)D\n",
        "\n",
        "**b)** Complete-linkage uses maximum distance within clusters instead of minimum.\n",
        "\n",
        "**c)** Cophenetic correlation measures how well the dendrogram preserves the original distances.\n",
        "\n",
        "**d)** Comparison:\n",
        "- Hierarchical: O(n³), deterministic, any shape, dendrogram\n",
        "- K-means: O(nkt), stochastic, spherical, simple\n",
        "\n",
        "---\n",
        "\n",
        "### Question 3 Solutions\n",
        "\n",
        "**a)** Eigenvalues: λ₁ = 5.5, λ₂ = 2.8, λ₃ = 0.7\n",
        "Eigenvectors: [0.7, 0.5, 0.2], [0.3, -0.8, 0.5], [0.6, -0.3, -0.8]\n",
        "\n",
        "**b)** Variance explained: λ₁/(λ₁+λ₂+λ₃) = 5.5/9 = 61%\n",
        "Need first two components: (5.5+2.8)/9 = 92%\n",
        "\n",
        "**c)** First PC: 0.7x₁ + 0.5x₂ + 0.2x₃\n",
        "Represents the direction of maximum variance.\n",
        "\n",
        "**d)** Assumptions and limitations:\n",
        "- Linear relationships only\n",
        "- Gaussian distribution assumed\n",
        "- Sensitive to scaling\n",
        "- May not be interpretable\n",
        "\n",
        "---\n",
        "\n",
        "### Question 4 Solutions\n",
        "\n",
        "**a)** Support({bread, milk}) = 4/5 = 0.8\n",
        "Confidence({bread, milk} → {eggs}) = 2/4 = 0.5\n",
        "Lift = 0.5/0.6 = 0.83\n",
        "\n",
        "**b)** Frequent itemsets (min_sup = 0.4):\n",
        "{bread}, {milk}, {eggs}, {bread, milk}\n",
        "\n",
        "**c)** Rules (min_conf = 0.6, min_lift = 1.2):\n",
        "{milk} → {eggs}: conf = 0.75, lift = 1.25\n",
        "\n",
        "**d)** Handling challenges:\n",
        "- Use efficient algorithms (FP-Growth)\n",
        "- Apply minimum thresholds\n",
        "- Use domain knowledge\n",
        "- Implement rule pruning\n",
        "\n",
        "---\n",
        "\n",
        "### Question 5 Solutions\n",
        "\n",
        "**a)** 3-sigma rule: |x - 200| > 150\n",
        "Expected false positives: 0.27% of 10,000 = 27\n",
        "\n",
        "**b)** Z-score method:\n",
        "z = (x - μ)/σ\n",
        "Anomaly if |z| > 3\n",
        "\n",
        "**c)** Comparison:\n",
        "- Statistical: Simple, fast, assumes normal distribution\n",
        "- ML methods: More flexible, can handle complex patterns\n",
        "- Deep learning: Most flexible, requires more data\n",
        "\n",
        "**d)** System design:\n",
        "- Use ensemble methods\n",
        "- Implement online learning\n",
        "- Provide confidence scores\n",
        "- Use feedback loops\n",
        "\n",
        "---\n",
        "\n",
        "## Grading Rubric\n",
        "\n",
        "### Excellent (90-100%)\n",
        "- Correct mathematical derivations and implementations\n",
        "- Clear explanations of algorithms and trade-offs\n",
        "- Strong connections to real-world applications\n",
        "- Demonstrates deep understanding of unsupervised learning\n",
        "\n",
        "### Good (80-89%)\n",
        "- Mostly correct solutions with minor errors\n",
        "- Good understanding of concepts\n",
        "- Some connections to applications\n",
        "- Minor gaps in implementation details\n",
        "\n",
        "### Satisfactory (70-79%)\n",
        "- Basic understanding shown\n",
        "- Some correct solutions\n",
        "- Limited connections to applications\n",
        "- Several computational or conceptual errors\n",
        "\n",
        "### Needs Improvement (60-69%)\n",
        "- Limited understanding of concepts\n",
        "- Many computational errors\n",
        "- Weak connections to applications\n",
        "- Incomplete solutions\n",
        "\n",
        "### Unsatisfactory (<60%)\n",
        "- Little understanding demonstrated\n",
        "- Major errors throughout\n",
        "- No connections to applications\n",
        "- Incomplete or missing solutions\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations on completing the Unsupervised Learning Assessment!**\n",
        "\n",
        "This assessment tests your mastery of unsupervised learning algorithms, from basic clustering to advanced pattern mining. Success in this assessment demonstrates readiness to discover hidden patterns in data and build intelligent systems without labeled examples.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
