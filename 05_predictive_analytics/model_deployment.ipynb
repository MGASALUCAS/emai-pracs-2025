{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Deployment: From Jupyter to Production\n",
        "\n",
        "> **\"A model is only as good as its deployment.\"**\n",
        "\n",
        "## Learning Objectives\n",
        "- Learn how to deploy machine learning models to production\n",
        "- Master model serialization and API development\n",
        "- Understand model monitoring and performance tracking\n",
        "- Implement containerization and cloud deployment\n",
        "- Apply MLOps best practices for production systems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Model Deployment Fundamentals\n",
        "\n",
        "### What is Model Deployment?\n",
        "Model deployment is the process of making a trained machine learning model available for use in production environments. It involves packaging the model, creating APIs, and ensuring the model can handle real-world data.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "#### 1. Model Serialization\n",
        "- **Pickle**: Python's built-in serialization\n",
        "- **Joblib**: Optimized for NumPy arrays\n",
        "- **ONNX**: Cross-platform model format\n",
        "- **TensorFlow SavedModel**: For TensorFlow models\n",
        "\n",
        "#### 2. API Development\n",
        "- **REST APIs**: HTTP-based interfaces\n",
        "- **GraphQL**: Flexible query language\n",
        "- **gRPC**: High-performance RPC framework\n",
        "\n",
        "#### 3. Monitoring and Maintenance\n",
        "- **Performance Monitoring**: Track accuracy, latency\n",
        "- **Data Drift Detection**: Monitor input distribution changes\n",
        "- **Model Retraining**: Update models with new data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample dataset for model deployment\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create synthetic dataset\n",
        "n_samples = 1000\n",
        "n_features = 10\n",
        "\n",
        "# Generate features\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "feature_names = [f'feature_{i}' for i in range(n_features)]\n",
        "\n",
        "# Create target variable\n",
        "true_coeffs = np.array([2.0, 1.5, -0.8, 0, 0, 0, 0, 0, 0, 0])\n",
        "y = X @ true_coeffs + np.random.normal(0, 0.5, n_samples)\n",
        "y_binary = (y > np.median(y)).astype(int)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['target'] = y_binary\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Dataset Overview:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
        "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
        "print(f\"Target distribution: {np.bincount(y_train)}\")\n",
        "\n",
        "# Train multiple models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
        "}\n",
        "\n",
        "trained_models = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    trained_models[name] = {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "    \n",
        "    print(f\"{name} Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Visualize model performance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Accuracy comparison\n",
        "model_names = list(trained_models.keys())\n",
        "accuracies = [trained_models[name]['accuracy'] for name in model_names]\n",
        "\n",
        "bars = axes[0].bar(model_names, accuracies, alpha=0.7)\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Model Performance Comparison')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for bar, accuracy in zip(bars, accuracies):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                 f'{accuracy:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Confusion matrix for best model\n",
        "best_model_name = max(trained_models.keys(), key=lambda x: trained_models[x]['accuracy'])\n",
        "best_model = trained_models[best_model_name]['model']\n",
        "y_pred_best = trained_models[best_model_name]['predictions']\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
        "axes[1].set_title(f'Confusion Matrix - {best_model_name}')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Serialization and Deployment\n",
        "print(\"Model Serialization and Deployment:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Select best model for deployment\n",
        "best_model_name = max(trained_models.keys(), key=lambda x: trained_models[x]['accuracy'])\n",
        "best_model = trained_models[best_model_name]['model']\n",
        "\n",
        "print(f\"Selected model for deployment: {best_model_name}\")\n",
        "print(f\"Model accuracy: {trained_models[best_model_name]['accuracy']:.3f}\")\n",
        "\n",
        "# Serialize model and scaler\n",
        "model_filename = 'best_model.joblib'\n",
        "scaler_filename = 'scaler.joblib'\n",
        "\n",
        "joblib.dump(best_model, model_filename)\n",
        "joblib.dump(scaler, scaler_filename)\n",
        "\n",
        "print(f\"\\nModel saved to: {model_filename}\")\n",
        "print(f\"Scaler saved to: {scaler_filename}\")\n",
        "\n",
        "# Create model metadata\n",
        "model_metadata = {\n",
        "    'model_name': best_model_name,\n",
        "    'model_type': type(best_model).__name__,\n",
        "    'accuracy': float(trained_models[best_model_name]['accuracy']),\n",
        "    'feature_names': feature_names,\n",
        "    'n_features': len(feature_names),\n",
        "    'training_date': '2024-01-01',\n",
        "    'version': '1.0.0'\n",
        "}\n",
        "\n",
        "# Save metadata\n",
        "with open('model_metadata.json', 'w') as f:\n",
        "    json.dump(model_metadata, f, indent=2)\n",
        "\n",
        "print(f\"Model metadata saved to: model_metadata.json\")\n",
        "\n",
        "# Load and test serialized model\n",
        "print(\"\\nTesting serialized model:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Load model and scaler\n",
        "loaded_model = joblib.load(model_filename)\n",
        "loaded_scaler = joblib.load(scaler_filename)\n",
        "\n",
        "# Test prediction\n",
        "test_sample = X_test_scaled[0:1]  # First test sample\n",
        "prediction = loaded_model.predict(test_sample)\n",
        "probability = loaded_model.predict_proba(test_sample)\n",
        "\n",
        "print(f\"Test sample prediction: {prediction[0]}\")\n",
        "print(f\"Prediction probability: {probability[0]}\")\n",
        "print(f\"Original accuracy: {trained_models[best_model_name]['accuracy']:.3f}\")\n",
        "\n",
        "# Verify model consistency\n",
        "all_predictions = loaded_model.predict(X_test_scaled)\n",
        "original_predictions = trained_models[best_model_name]['predictions']\n",
        "predictions_match = np.array_equal(all_predictions, original_predictions)\n",
        "\n",
        "print(f\"Predictions match original: {predictions_match}\")\n",
        "\n",
        "# Model deployment simulation\n",
        "print(\"\\nModel Deployment Simulation:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "class ModelPredictor:\n",
        "    \"\"\"Simple model predictor class for deployment.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_path, scaler_path, metadata_path):\n",
        "        self.model = joblib.load(model_path)\n",
        "        self.scaler = joblib.load(scaler_path)\n",
        "        \n",
        "        with open(metadata_path, 'r') as f:\n",
        "            self.metadata = json.load(f)\n",
        "    \n",
        "    def predict(self, features):\n",
        "        \"\"\"Make prediction on new data.\"\"\"\n",
        "        # Validate input\n",
        "        if len(features) != self.metadata['n_features']:\n",
        "            raise ValueError(f\"Expected {self.metadata['n_features']} features, got {len(features)}\")\n",
        "        \n",
        "        # Scale features\n",
        "        features_scaled = self.scaler.transform([features])\n",
        "        \n",
        "        # Make prediction\n",
        "        prediction = self.model.predict(features_scaled)[0]\n",
        "        probability = self.model.predict_proba(features_scaled)[0]\n",
        "        \n",
        "        return {\n",
        "            'prediction': int(prediction),\n",
        "            'probability': float(probability[1]),  # Probability of class 1\n",
        "            'confidence': float(max(probability))\n",
        "        }\n",
        "    \n",
        "    def get_model_info(self):\n",
        "        \"\"\"Get model information.\"\"\"\n",
        "        return self.metadata\n",
        "\n",
        "# Initialize predictor\n",
        "predictor = ModelPredictor(model_filename, scaler_filename, 'model_metadata.json')\n",
        "\n",
        "# Test with new data\n",
        "print(\"Testing with new data:\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Generate new sample\n",
        "new_sample = np.random.randn(10)  # 10 features\n",
        "result = predictor.predict(new_sample)\n",
        "\n",
        "print(f\"New sample: {new_sample}\")\n",
        "print(f\"Prediction: {result['prediction']}\")\n",
        "print(f\"Probability: {result['probability']:.3f}\")\n",
        "print(f\"Confidence: {result['confidence']:.3f}\")\n",
        "\n",
        "# Model information\n",
        "print(f\"\\nModel Information:\")\n",
        "print(\"-\" * 20)\n",
        "model_info = predictor.get_model_info()\n",
        "for key, value in model_info.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Performance monitoring simulation\n",
        "print(f\"\\nPerformance Monitoring:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Simulate monitoring over time\n",
        "n_days = 30\n",
        "daily_accuracy = []\n",
        "daily_predictions = []\n",
        "\n",
        "for day in range(n_days):\n",
        "    # Simulate daily predictions\n",
        "    n_predictions = np.random.randint(50, 200)\n",
        "    daily_pred = []\n",
        "    \n",
        "    for _ in range(n_predictions):\n",
        "        # Generate random sample\n",
        "        sample = np.random.randn(10)\n",
        "        pred = predictor.predict(sample)\n",
        "        daily_pred.append(pred['prediction'])\n",
        "    \n",
        "    # Simulate accuracy (with some noise)\n",
        "    base_accuracy = trained_models[best_model_name]['accuracy']\n",
        "    noise = np.random.normal(0, 0.05)\n",
        "    daily_acc = max(0, min(1, base_accuracy + noise))\n",
        "    \n",
        "    daily_accuracy.append(daily_acc)\n",
        "    daily_predictions.append(n_predictions)\n",
        "\n",
        "# Plot monitoring results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Daily accuracy\n",
        "axes[0].plot(range(1, n_days + 1), daily_accuracy, 'b-', alpha=0.7)\n",
        "axes[0].axhline(y=base_accuracy, color='r', linestyle='--', label='Baseline Accuracy')\n",
        "axes[0].set_xlabel('Day')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Daily Model Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Daily predictions\n",
        "axes[1].bar(range(1, n_days + 1), daily_predictions, alpha=0.7)\n",
        "axes[1].set_xlabel('Day')\n",
        "axes[1].set_ylabel('Number of Predictions')\n",
        "axes[1].set_title('Daily Prediction Volume')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Average daily accuracy: {np.mean(daily_accuracy):.3f}\")\n",
        "print(f\"Average daily predictions: {np.mean(daily_predictions):.1f}\")\n",
        "print(f\"Total predictions over {n_days} days: {sum(daily_predictions)}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
