{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End-to-End Predictive Analytics Pipeline\n",
        "\n",
        "> **\"The best way to learn predictive analytics is to build complete systems.\"**\n",
        "\n",
        "## Learning Objectives\n",
        "- Master the complete data science pipeline from raw data to deployment\n",
        "- Learn advanced feature engineering and selection techniques\n",
        "- Implement model evaluation and validation strategies\n",
        "- Build production-ready ML systems\n",
        "- Apply best practices for real-world ML projects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Complete Data Science Pipeline\n",
        "\n",
        "### 1. Problem Definition\n",
        "- Define business objective\n",
        "- Identify success metrics\n",
        "- Determine data requirements\n",
        "\n",
        "### 2. Data Collection & Integration\n",
        "- Gather data from multiple sources\n",
        "- Handle different data formats\n",
        "- Ensure data quality\n",
        "\n",
        "### 3. Data Preprocessing\n",
        "- Handle missing values\n",
        "- Detect and treat outliers\n",
        "- Encode categorical variables\n",
        "- Scale numerical features\n",
        "\n",
        "### 4. Feature Engineering\n",
        "- Create new features\n",
        "- Select relevant features\n",
        "- Handle feature interactions\n",
        "\n",
        "### 5. Model Development\n",
        "- Choose appropriate algorithms\n",
        "- Train and validate models\n",
        "- Tune hyperparameters\n",
        "\n",
        "### 6. Model Deployment\n",
        "- Deploy to production\n",
        "- Monitor performance\n",
        "- Update models regularly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample dataset for our end-to-end pipeline\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a realistic dataset\n",
        "n_samples = 1000\n",
        "n_features = 10\n",
        "\n",
        "# Generate features\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Add some feature names for better understanding\n",
        "feature_names = [f'feature_{i}' for i in range(n_features)]\n",
        "\n",
        "# Create target variable with some relationship to features\n",
        "# y = 2*feature_0 + 1.5*feature_1 - 0.8*feature_2 + noise\n",
        "true_coeffs = np.array([2.0, 1.5, -0.8, 0, 0, 0, 0, 0, 0, 0])  # Only first 3 features matter\n",
        "y = X @ true_coeffs + np.random.normal(0, 0.5, n_samples)\n",
        "\n",
        "# Add some missing values (5% missing)\n",
        "missing_mask = np.random.random((n_samples, n_features)) < 0.05\n",
        "X[missing_mask] = np.nan\n",
        "\n",
        "# Add some outliers (2% of samples)\n",
        "outlier_mask = np.random.random(n_samples) < 0.02\n",
        "X[outlier_mask, 0] += np.random.normal(0, 5, np.sum(outlier_mask))\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "print(\"Dataset Overview:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\nTarget statistics:\")\n",
        "print(df['target'].describe())\n",
        "\n",
        "# Visualize the data\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Target distribution\n",
        "axes[0, 0].hist(df['target'], bins=30, alpha=0.7, edgecolor='black')\n",
        "axes[0, 0].set_title('Target Variable Distribution')\n",
        "axes[0, 0].set_xlabel('Target Value')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "\n",
        "# Feature correlations\n",
        "correlation_matrix = df.corr()\n",
        "im = axes[0, 1].imshow(correlation_matrix, cmap='coolwarm', aspect='auto')\n",
        "axes[0, 1].set_title('Feature Correlation Matrix')\n",
        "axes[0, 1].set_xticks(range(len(feature_names)))\n",
        "axes[0, 1].set_yticks(range(len(feature_names)))\n",
        "axes[0, 1].set_xticklabels(feature_names, rotation=45)\n",
        "axes[0, 1].set_yticklabels(feature_names)\n",
        "plt.colorbar(im, ax=axes[0, 1])\n",
        "\n",
        "# Missing values heatmap\n",
        "missing_data = df.isnull()\n",
        "axes[1, 0].imshow(missing_data.T, cmap='viridis', aspect='auto')\n",
        "axes[1, 0].set_title('Missing Values Pattern')\n",
        "axes[1, 0].set_xlabel('Sample Index')\n",
        "axes[1, 0].set_ylabel('Features')\n",
        "\n",
        "# Feature distributions\n",
        "df.iloc[:, :5].boxplot(ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Feature Distributions (First 5)')\n",
        "axes[1, 1].set_ylabel('Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing Pipeline\n",
        "\n",
        "### Step 1: Handle Missing Values\n",
        "- **Strategy**: Use median imputation for numerical features\n",
        "- **Reasoning**: Median is robust to outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Handle Missing Values\n",
        "print(\"Step 1: Handling Missing Values\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Check missing values before imputation\n",
        "print(\"Missing values before imputation:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Impute missing values with median\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Convert back to DataFrame\n",
        "X_imputed_df = pd.DataFrame(X_imputed, columns=feature_names)\n",
        "\n",
        "print(f\"\\nMissing values after imputation:\")\n",
        "print(X_imputed_df.isnull().sum().sum())\n",
        "\n",
        "# Step 2: Detect and Handle Outliers\n",
        "print(\"\\nStep 2: Detecting Outliers\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Detect outliers\n",
        "outlier_detector = IsolationForest(contamination=0.1, random_state=42)\n",
        "outlier_labels = outlier_detector.fit_predict(X_imputed)\n",
        "\n",
        "# Count outliers\n",
        "n_outliers = np.sum(outlier_labels == -1)\n",
        "print(f\"Number of outliers detected: {n_outliers}\")\n",
        "\n",
        "# Visualize outliers\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot first two features with outliers highlighted\n",
        "axes[0].scatter(X_imputed_df.iloc[outlier_labels == 1, 0], \n",
        "                X_imputed_df.iloc[outlier_labels == 1, 1], \n",
        "                alpha=0.6, label='Normal', s=50)\n",
        "axes[0].scatter(X_imputed_df.iloc[outlier_labels == -1, 0], \n",
        "                X_imputed_df.iloc[outlier_labels == -1, 1], \n",
        "                alpha=0.8, label='Outliers', s=50, color='red')\n",
        "axes[0].set_xlabel('Feature 0')\n",
        "axes[0].set_ylabel('Feature 1')\n",
        "axes[0].set_title('Outlier Detection')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Box plot showing outliers\n",
        "X_imputed_df.iloc[:, :5].boxplot(ax=axes[1])\n",
        "axes[1].set_title('Feature Distributions with Outliers')\n",
        "axes[1].set_ylabel('Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Remove outliers\n",
        "X_clean = X_imputed_df[outlier_labels == 1]\n",
        "y_clean = y[outlier_labels == 1]\n",
        "\n",
        "print(f\"Dataset shape after removing outliers: {X_clean.shape}\")\n",
        "print(f\"Removed {len(X_imputed_df) - len(X_clean)} outliers\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
