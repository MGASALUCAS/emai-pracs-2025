{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering: The Art of Creating Better Features\n",
        "\n",
        "> **\"Feature engineering is where domain expertise meets machine learning.\"**\n",
        "\n",
        "## Learning Objectives\n",
        "- Master advanced feature engineering techniques\n",
        "- Learn about feature selection and dimensionality reduction\n",
        "- Understand handling of different data types (numerical, categorical, text)\n",
        "- Implement automated feature engineering pipelines\n",
        "- Apply feature engineering to real-world datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Feature Engineering Fundamentals\n",
        "\n",
        "### What is Feature Engineering?\n",
        "Feature engineering is the process of creating new features or transforming existing ones to improve machine learning model performance. It's often considered the most important step in the machine learning pipeline.\n",
        "\n",
        "### Key Techniques\n",
        "\n",
        "#### 1. Feature Scaling\n",
        "- **Standardization**: (x - μ) / σ\n",
        "- **Min-Max Scaling**: (x - min) / (max - min)\n",
        "- **Robust Scaling**: (x - median) / IQR\n",
        "\n",
        "#### 2. Feature Selection\n",
        "- **Filter Methods**: Statistical tests, correlation analysis\n",
        "- **Wrapper Methods**: Forward/backward selection, recursive feature elimination\n",
        "- **Embedded Methods**: L1 regularization, tree-based feature importance\n",
        "\n",
        "#### 3. Feature Creation\n",
        "- **Polynomial Features**: x², x³, interactions\n",
        "- **Binning**: Convert continuous to categorical\n",
        "- **Domain Knowledge**: Create features based on business logic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample dataset for feature engineering\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create synthetic dataset\n",
        "n_samples = 1000\n",
        "n_features = 10\n",
        "\n",
        "# Generate features\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Add some feature names\n",
        "feature_names = [f'feature_{i}' for i in range(n_features)]\n",
        "\n",
        "# Create target variable with some relationship to features\n",
        "# y = 2*feature_0 + 1.5*feature_1 - 0.8*feature_2 + noise\n",
        "true_coeffs = np.array([2.0, 1.5, -0.8, 0, 0, 0, 0, 0, 0, 0])  # Only first 3 features matter\n",
        "y = X @ true_coeffs + np.random.normal(0, 0.5, n_samples)\n",
        "\n",
        "# Convert to binary classification\n",
        "y_binary = (y > np.median(y)).astype(int)\n",
        "\n",
        "# Add some missing values (5% missing)\n",
        "missing_mask = np.random.random((n_samples, n_features)) < 0.05\n",
        "X[missing_mask] = np.nan\n",
        "\n",
        "# Add some outliers (2% of samples)\n",
        "outlier_mask = np.random.random(n_samples) < 0.02\n",
        "X[outlier_mask, 0] += np.random.normal(0, 5, np.sum(outlier_mask))\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['target'] = y_binary\n",
        "\n",
        "print(\"Dataset Overview:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(df['target'].value_counts())\n",
        "\n",
        "# Visualize the data\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Target distribution\n",
        "axes[0, 0].hist(df['target'], bins=2, alpha=0.7, edgecolor='black')\n",
        "axes[0, 0].set_title('Target Distribution')\n",
        "axes[0, 0].set_xlabel('Target')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "\n",
        "# Feature correlations\n",
        "correlation_matrix = df.corr()\n",
        "im = axes[0, 1].imshow(correlation_matrix, cmap='coolwarm', aspect='auto')\n",
        "axes[0, 1].set_title('Feature Correlation Matrix')\n",
        "axes[0, 1].set_xticks(range(len(feature_names)))\n",
        "axes[0, 1].set_yticks(range(len(feature_names)))\n",
        "axes[0, 1].set_xticklabels(feature_names, rotation=45)\n",
        "axes[0, 1].set_yticklabels(feature_names)\n",
        "plt.colorbar(im, ax=axes[0, 1])\n",
        "\n",
        "# Missing values heatmap\n",
        "missing_data = df.isnull()\n",
        "axes[1, 0].imshow(missing_data.T, cmap='viridis', aspect='auto')\n",
        "axes[1, 0].set_title('Missing Values Pattern')\n",
        "axes[1, 0].set_xlabel('Sample Index')\n",
        "axes[1, 0].set_ylabel('Features')\n",
        "\n",
        "# Feature distributions\n",
        "df.iloc[:, :5].boxplot(ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Feature Distributions (First 5)')\n",
        "axes[1, 1].set_ylabel('Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering Pipeline\n",
        "print(\"Feature Engineering Pipeline:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Step 1: Handle Missing Values\n",
        "print(\"Step 1: Handling Missing Values\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Check missing values before imputation\n",
        "print(\"Missing values before imputation:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Impute missing values with median\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Convert back to DataFrame\n",
        "X_imputed_df = pd.DataFrame(X_imputed, columns=feature_names)\n",
        "\n",
        "print(f\"\\nMissing values after imputation:\")\n",
        "print(X_imputed_df.isnull().sum().sum())\n",
        "\n",
        "# Step 2: Feature Scaling\n",
        "print(\"\\nStep 2: Feature Scaling\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Compare different scaling methods\n",
        "scalers = {\n",
        "    'Original': None,\n",
        "    'StandardScaler': StandardScaler(),\n",
        "    'MinMaxScaler': MinMaxScaler()\n",
        "}\n",
        "\n",
        "scaled_data = {}\n",
        "for name, scaler in scalers.items():\n",
        "    if scaler is None:\n",
        "        scaled_data[name] = X_imputed\n",
        "    else:\n",
        "        scaled_data[name] = scaler.fit_transform(X_imputed)\n",
        "\n",
        "# Visualize scaling effects\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for i, (name, data) in enumerate(scaled_data.items()):\n",
        "    axes[i].boxplot(data[:, :5], labels=feature_names[:5])\n",
        "    axes[i].set_title(f'{name} Scaling')\n",
        "    axes[i].set_ylabel('Value')\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Feature Selection\n",
        "print(\"\\nStep 3: Feature Selection\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Use StandardScaler for feature selection\n",
        "X_scaled = StandardScaler().fit_transform(X_imputed)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature selection methods\n",
        "selection_methods = {\n",
        "    'F-test': SelectKBest(f_classif, k=5),\n",
        "    'Mutual Information': SelectKBest(mutual_info_classif, k=5)\n",
        "}\n",
        "\n",
        "selected_features = {}\n",
        "for name, selector in selection_methods.items():\n",
        "    X_selected = selector.fit_transform(X_train, y_train)\n",
        "    selected_features[name] = X_selected\n",
        "    \n",
        "    # Get selected feature indices\n",
        "    selected_indices = selector.get_support(indices=True)\n",
        "    selected_feature_names = [feature_names[i] for i in selected_indices]\n",
        "    \n",
        "    print(f\"{name} selected features: {selected_feature_names}\")\n",
        "\n",
        "# Step 4: Feature Creation\n",
        "print(\"\\nStep 4: Feature Creation\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Create polynomial features\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Create polynomial features (degree 2)\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Original features: {X_scaled.shape[1]}\")\n",
        "print(f\"Polynomial features: {X_poly.shape[1]}\")\n",
        "\n",
        "# Create interaction features\n",
        "X_interaction = X_scaled.copy()\n",
        "for i in range(X_scaled.shape[1]):\n",
        "    for j in range(i+1, X_scaled.shape[1]):\n",
        "        interaction_feature = X_scaled[:, i] * X_scaled[:, j]\n",
        "        X_interaction = np.column_stack([X_interaction, interaction_feature])\n",
        "\n",
        "print(f\"With interaction features: {X_interaction.shape[1]}\")\n",
        "\n",
        "# Step 5: Model Performance Comparison\n",
        "print(\"\\nStep 5: Model Performance Comparison\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Train models on different feature sets\n",
        "models = {\n",
        "    'Original': X_scaled,\n",
        "    'F-test Selected': selected_features['F-test'],\n",
        "    'Mutual Info Selected': selected_features['Mutual Information'],\n",
        "    'Polynomial': X_poly,\n",
        "    'With Interactions': X_interaction\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, X_data in models.items():\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Train model\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    results[name] = accuracy\n",
        "    print(f\"{name}: {accuracy:.3f}\")\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(12, 6))\n",
        "names = list(results.keys())\n",
        "accuracies = list(results.values())\n",
        "\n",
        "bars = plt.bar(names, accuracies, alpha=0.7)\n",
        "plt.xlabel('Feature Set')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Performance with Different Feature Sets')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, accuracy in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{accuracy:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
