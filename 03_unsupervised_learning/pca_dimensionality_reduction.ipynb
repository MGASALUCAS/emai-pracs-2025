{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCA: Dimensionality Reduction and Data Compression\n",
        "\n",
        "> **\"PCA finds the most important directions in your data.\"**\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the mathematical foundation of Principal Component Analysis\n",
        "- Implement PCA from scratch using eigenvalue decomposition\n",
        "- Learn about variance explained and component selection\n",
        "- Master visualization techniques for high-dimensional data\n",
        "- Apply PCA to real-world dimensionality reduction problems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, load_iris, load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Principal Component Analysis (PCA) Fundamentals\n",
        "\n",
        "### What is PCA?\n",
        "PCA is a dimensionality reduction technique that transforms data into a new coordinate system where the first coordinate (first principal component) has the largest possible variance, the second coordinate has the second largest variance, and so on.\n",
        "\n",
        "### Mathematical Foundation\n",
        "\n",
        "#### 1. Covariance Matrix\n",
        "$C = \\frac{1}{n-1}X^TX$\n",
        "\n",
        "Where X is the centered data matrix.\n",
        "\n",
        "#### 2. Eigenvalue Decomposition\n",
        "$C = V\\Lambda V^T$\n",
        "\n",
        "Where:\n",
        "- V contains the eigenvectors (principal components)\n",
        "- Î› contains the eigenvalues (variances)\n",
        "\n",
        "#### 3. Transformation\n",
        "$Y = XV$\n",
        "\n",
        "Where Y is the transformed data in the new coordinate system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample data\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=3, \n",
        "                          n_redundant=7, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Dataset Overview:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Original shape: {X.shape}\")\n",
        "print(f\"Scaled shape: {X_scaled.shape}\")\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Calculate explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "print(f\"\\nExplained Variance Ratio:\")\n",
        "print(\"=\" * 50)\n",
        "for i, (var_ratio, cum_var_ratio) in enumerate(zip(explained_variance_ratio, cumulative_variance_ratio)):\n",
        "    print(f\"PC{i+1}: {var_ratio:.3f} (Cumulative: {cum_var_ratio:.3f})\")\n",
        "\n",
        "# Visualize explained variance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Individual explained variance\n",
        "axes[0].bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.7)\n",
        "axes[0].set_xlabel('Principal Component')\n",
        "axes[0].set_ylabel('Explained Variance Ratio')\n",
        "axes[0].set_title('Explained Variance by Component')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Cumulative explained variance\n",
        "axes[1].plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'o-', linewidth=2)\n",
        "axes[1].axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95% variance')\n",
        "axes[1].axhline(y=0.99, color='g', linestyle='--', alpha=0.7, label='99% variance')\n",
        "axes[1].set_xlabel('Number of Components')\n",
        "axes[1].set_ylabel('Cumulative Explained Variance Ratio')\n",
        "axes[1].set_title('Cumulative Explained Variance')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find number of components for 95% and 99% variance\n",
        "n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
        "n_components_99 = np.argmax(cumulative_variance_ratio >= 0.99) + 1\n",
        "\n",
        "print(f\"\\nNumber of components for 95% variance: {n_components_95}\")\n",
        "print(f\"Number of components for 99% variance: {n_components_99}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply PCA with different numbers of components\n",
        "n_components_list = [2, 3, 5, 7, 10]\n",
        "results = {}\n",
        "\n",
        "for n_components in n_components_list:\n",
        "    # Apply PCA\n",
        "    pca_reduced = PCA(n_components=n_components)\n",
        "    X_pca_reduced = pca_reduced.fit_transform(X_scaled)\n",
        "    \n",
        "    # Train classifier on reduced data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_pca_reduced, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    clf = LogisticRegression(random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
        "    \n",
        "    # Calculate explained variance\n",
        "    explained_variance = np.sum(pca_reduced.explained_variance_ratio_)\n",
        "    \n",
        "    results[n_components] = {\n",
        "        'accuracy': accuracy,\n",
        "        'explained_variance': explained_variance,\n",
        "        'data_shape': X_pca_reduced.shape\n",
        "    }\n",
        "\n",
        "# Visualize results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Accuracy vs number of components\n",
        "n_components = list(results.keys())\n",
        "accuracies = [results[n]['accuracy'] for n in n_components]\n",
        "explained_variances = [results[n]['explained_variance'] for n in n_components]\n",
        "\n",
        "axes[0].plot(n_components, accuracies, 'o-', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('Number of Components')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Classification Accuracy vs Number of Components')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for i, (n_comp, acc) in enumerate(zip(n_components, accuracies)):\n",
        "    axes[0].text(n_comp, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Explained variance vs number of components\n",
        "axes[1].plot(n_components, explained_variances, 's-', linewidth=2, markersize=8, color='green')\n",
        "axes[1].set_xlabel('Number of Components')\n",
        "axes[1].set_ylabel('Explained Variance Ratio')\n",
        "axes[1].set_title('Explained Variance vs Number of Components')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for i, (n_comp, var) in enumerate(zip(n_components, explained_variances)):\n",
        "    axes[1].text(n_comp, var + 0.01, f'{var:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print results table\n",
        "print(\"\\nResults Summary:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Components':<12} {'Accuracy':<10} {'Explained Var':<15} {'Data Shape':<15}\")\n",
        "print(\"-\" * 80)\n",
        "for n_comp in n_components:\n",
        "    result = results[n_comp]\n",
        "    print(f\"{n_comp:<12} {result['accuracy']:<10.3f} {result['explained_variance']:<15.3f} {str(result['data_shape']):<15}\")\n",
        "\n",
        "# Visualize 2D projection\n",
        "pca_2d = PCA(n_components=2)\n",
        "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
        "plt.xlabel(f'First Principal Component ({pca_2d.explained_variance_ratio_[0]:.3f})')\n",
        "plt.ylabel(f'Second Principal Component ({pca_2d.explained_variance_ratio_[1]:.3f})')\n",
        "plt.title('2D PCA Projection')\n",
        "plt.colorbar(scatter)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
