{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Algebra Essentials for Machine Learning\n",
        "\n",
        "> **\"The best way to learn linear algebra is to visualize it.\"**\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand vectors as geometric objects and algebraic entities\n",
        "- Master matrix operations and their geometric interpretations\n",
        "- Build intuition for linear transformations\n",
        "- Connect linear algebra concepts to machine learning applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Vectors: The Building Blocks\n",
        "\n",
        "### What is a Vector?\n",
        "A vector is an ordered list of numbers that can represent:\n",
        "- **Position**: A point in space\n",
        "- **Direction**: Which way to go\n",
        "- **Magnitude**: How far to go\n",
        "\n",
        "In machine learning, vectors often represent:\n",
        "- **Features**: Characteristics of data points\n",
        "- **Weights**: Parameters of our model\n",
        "- **Gradients**: Directions for optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create a 2D vector\n",
        "v = np.array([3, 4])\n",
        "print(f\"Vector v = {v}\")\n",
        "print(f\"Magnitude (length) of v = {np.linalg.norm(v):.2f}\")\n",
        "print(f\"Direction (unit vector) = {v / np.linalg.norm(v)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the vector\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "\n",
        "# Plot the vector from origin\n",
        "ax.arrow(0, 0, v[0], v[1], head_width=0.3, head_length=0.3, fc='red', ec='red', linewidth=2)\n",
        "ax.set_xlim(-1, 5)\n",
        "ax.set_ylim(-1, 5)\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_title('Vector v = [3, 4]')\n",
        "\n",
        "# Add annotations\n",
        "ax.annotate(f'v = [{v[0]}, {v[1]}]', xy=(v[0], v[1]), xytext=(v[0]+0.5, v[1]+0.5),\n",
        "            arrowprops=dict(arrowstyle='->', color='blue', alpha=0.7))\n",
        "ax.annotate(f'Magnitude = {np.linalg.norm(v):.2f}', xy=(1.5, 2), fontsize=12, color='green')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector Operations\n",
        "\n",
        "#### 1. Vector Addition\n",
        "Adding vectors means moving in the direction of the first vector, then the second.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vector addition\n",
        "v1 = np.array([2, 1])\n",
        "v2 = np.array([1, 3])\n",
        "v_sum = v1 + v2\n",
        "\n",
        "print(f\"v1 = {v1}\")\n",
        "print(f\"v2 = {v2}\")\n",
        "print(f\"v1 + v2 = {v_sum}\")\n",
        "\n",
        "# Visualize vector addition\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "\n",
        "# Plot v1\n",
        "ax.arrow(0, 0, v1[0], v1[1], head_width=0.2, head_length=0.2, fc='blue', ec='blue', linewidth=2, label='v1')\n",
        "\n",
        "# Plot v2 from the end of v1\n",
        "ax.arrow(v1[0], v1[1], v2[0], v2[1], head_width=0.2, head_length=0.2, fc='green', ec='green', linewidth=2, label='v2')\n",
        "\n",
        "# Plot v2 from origin (to show parallelogram)\n",
        "ax.arrow(0, 0, v2[0], v2[1], head_width=0.2, head_length=0.2, fc='green', ec='green', linewidth=2, alpha=0.5, linestyle='--')\n",
        "\n",
        "# Plot the sum\n",
        "ax.arrow(0, 0, v_sum[0], v_sum[1], head_width=0.3, head_length=0.3, fc='red', ec='red', linewidth=3, label='v1 + v2')\n",
        "\n",
        "ax.set_xlim(-0.5, 4)\n",
        "ax.set_ylim(-0.5, 5)\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend()\n",
        "ax.set_title('Vector Addition: v1 + v2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Scalar Multiplication\n",
        "Multiplying a vector by a scalar changes its magnitude but not its direction (unless the scalar is negative).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scalar multiplication\n",
        "v = np.array([2, 1])\n",
        "scalar = 2.5\n",
        "v_scaled = scalar * v\n",
        "\n",
        "print(f\"Original vector v = {v}\")\n",
        "print(f\"Scalar = {scalar}\")\n",
        "print(f\"Scaled vector = {v_scaled}\")\n",
        "print(f\"Original magnitude = {np.linalg.norm(v):.2f}\")\n",
        "print(f\"Scaled magnitude = {np.linalg.norm(v_scaled):.2f}\")\n",
        "\n",
        "# Visualize scalar multiplication\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "# Plot original vector\n",
        "ax.arrow(0, 0, v[0], v[1], head_width=0.2, head_length=0.2, fc='blue', ec='blue', linewidth=2, label=f'v = {v}')\n",
        "\n",
        "# Plot scaled vector\n",
        "ax.arrow(0, 0, v_scaled[0], v_scaled[1], head_width=0.3, head_length=0.3, fc='red', ec='red', linewidth=2, label=f'{scalar}v = {v_scaled}')\n",
        "\n",
        "ax.set_xlim(-1, 6)\n",
        "ax.set_ylim(-1, 3)\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend()\n",
        "ax.set_title('Scalar Multiplication')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Dot Product (Inner Product)\n",
        "The dot product measures how much two vectors point in the same direction.\n",
        "\n",
        "**Mathematical definition:** $\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_i b_i$\n",
        "\n",
        "**Geometric interpretation:** $\\vec{a} \\cdot \\vec{b} = |\\vec{a}||\\vec{b}|\\cos(\\theta)$\n",
        "\n",
        "Where $\\theta$ is the angle between the vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dot product examples\n",
        "a = np.array([3, 4])\n",
        "b = np.array([1, 2])\n",
        "\n",
        "# Method 1: Using numpy.dot()\n",
        "dot_product1 = np.dot(a, b)\n",
        "\n",
        "# Method 2: Using @ operator\n",
        "dot_product2 = a @ b\n",
        "\n",
        "# Method 3: Manual calculation\n",
        "dot_product3 = np.sum(a * b)\n",
        "\n",
        "print(f\"Vector a = {a}\")\n",
        "print(f\"Vector b = {b}\")\n",
        "print(f\"Dot product (method 1) = {dot_product1}\")\n",
        "print(f\"Dot product (method 2) = {dot_product2}\")\n",
        "print(f\"Dot product (method 3) = {dot_product3}\")\n",
        "\n",
        "# Calculate angle between vectors\n",
        "cos_theta = dot_product1 / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "theta_radians = np.arccos(cos_theta)\n",
        "theta_degrees = np.degrees(theta_radians)\n",
        "\n",
        "print(f\"\\nAngle between vectors: {theta_degrees:.2f} degrees\")\n",
        "print(f\"Cosine of angle: {cos_theta:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Matrices: The Power Tools\n",
        "\n",
        "### What is a Matrix?\n",
        "A matrix is a rectangular array of numbers arranged in rows and columns.\n",
        "\n",
        "In machine learning, matrices represent:\n",
        "- **Data**: Each row is a sample, each column is a feature\n",
        "- **Transformations**: How to change data from one space to another\n",
        "- **Weights**: Parameters of neural networks\n",
        "- **Covariance**: Relationships between features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating matrices\n",
        "A = np.array([[1, 2, 3],\n",
        "              [4, 5, 6]])\n",
        "\n",
        "B = np.array([[7, 8],\n",
        "              [9, 10],\n",
        "              [11, 12]])\n",
        "\n",
        "print(f\"Matrix A (2x3):\\n{A}\")\n",
        "print(f\"\\nMatrix B (3x2):\\n{B}\")\n",
        "print(f\"\\nShape of A: {A.shape}\")\n",
        "print(f\"Shape of B: {B.shape}\")\n",
        "\n",
        "# Matrix multiplication\n",
        "C = np.dot(A, B)  # or A @ B\n",
        "print(f\"\\nA @ B =\\n{C}\")\n",
        "print(f\"\\nShape of result: {C.shape}\")\n",
        "\n",
        "# Manual calculation to understand the process\n",
        "print(\"\\nManual calculation:\")\n",
        "print(f\"C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] + A[0,2]*B[2,0] = {A[0,0]}*{B[0,0]} + {A[0,1]}*{B[1,0]} + {A[0,2]}*{B[2,0]} = {A[0,0]*B[0,0] + A[0,1]*B[1,0] + A[0,2]*B[2,0]}\")\n",
        "print(f\"C[0,1] = A[0,0]*B[0,1] + A[0,1]*B[1,1] + A[0,2]*B[2,1] = {A[0,0]}*{B[0,1]} + {A[0,1]}*{B[1,1]} + {A[0,2]}*{B[2,1]} = {A[0,0]*B[0,1] + A[0,1]*B[1,1] + A[0,2]*B[2,1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Applications in Machine Learning\n",
        "\n",
        "### 1. Principal Component Analysis (PCA)\n",
        "PCA finds the directions of maximum variance in data using eigenvectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample data\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "X = np.random.multivariate_normal([0, 0], [[3, 1], [1, 1]], n_samples)\n",
        "\n",
        "# Calculate covariance matrix\n",
        "cov_matrix = np.cov(X.T)\n",
        "print(f\"Covariance matrix:\\n{cov_matrix}\")\n",
        "\n",
        "# Find eigenvalues and eigenvectors\n",
        "eigenvals, eigenvecs = np.linalg.eig(cov_matrix)\n",
        "\n",
        "# Sort by eigenvalue (descending)\n",
        "idx = np.argsort(eigenvals)[::-1]\n",
        "eigenvals = eigenvals[idx]\n",
        "eigenvecs = eigenvecs[:, idx]\n",
        "\n",
        "print(f\"\\nEigenvalues: {eigenvals}\")\n",
        "print(f\"Eigenvectors:\\n{eigenvecs}\")\n",
        "\n",
        "# Visualize PCA\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "\n",
        "# Plot data points\n",
        "ax.scatter(X[:, 0], X[:, 1], alpha=0.6, s=50, label='Data points')\n",
        "\n",
        "# Plot eigenvectors (scaled by eigenvalues)\n",
        "center = np.mean(X, axis=0)\n",
        "for i, (eigenval, eigenvec) in enumerate(zip(eigenvals, eigenvecs.T)):\n",
        "    scale = 2 * np.sqrt(eigenval)  # Scale by standard deviation\n",
        "    ax.arrow(center[0], center[1], \n",
        "             eigenvec[0] * scale, eigenvec[1] * scale,\n",
        "             head_width=0.3, head_length=0.3, \n",
        "             fc=f'C{i+1}', ec=f'C{i+1}', linewidth=3,\n",
        "             label=f'PC{i+1} (λ={eigenval:.2f})')\n",
        "\n",
        "ax.set_xlabel('Feature 1')\n",
        "ax.set_ylabel('Feature 2')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_title('Principal Component Analysis')\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Linear Regression\n",
        "Linear regression uses matrix operations to find the best line through data points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample data for linear regression\n",
        "np.random.seed(42)\n",
        "n_points = 50\n",
        "x = np.linspace(0, 10, n_points)\n",
        "y = 2 * x + 3 + np.random.normal(0, 2, n_points)  # y = 2x + 3 + noise\n",
        "\n",
        "# Prepare data for matrix operations\n",
        "X = np.column_stack([np.ones(n_points), x])  # Add bias term\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "# Normal equation: θ = (X^T X)^(-1) X^T y\n",
        "theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "\n",
        "print(f\"True parameters: slope=2, intercept=3\")\n",
        "print(f\"Estimated parameters: slope={theta[1,0]:.2f}, intercept={theta[0,0]:.2f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = X @ theta\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x, y, alpha=0.6, s=50, label='Data points')\n",
        "plt.plot(x, y_pred, 'r-', linewidth=2, label=f'Linear fit: y = {theta[1,0]:.2f}x + {theta[0,0]:.2f}')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.title('Linear Regression using Matrix Operations')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Vectors** are the building blocks of machine learning - they represent features, weights, and data points\n",
        "2. **Matrices** are powerful tools for transformations, data representation, and computations\n",
        "3. **Linear transformations** help us understand how data changes in different spaces\n",
        "4. **Eigenvalues and eigenvectors** reveal the fundamental structure of data\n",
        "5. **Matrix operations** are the computational foundation of most ML algorithms\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Now that you understand linear algebra, you're ready to explore:\n",
        "- **Probability and Statistics** - How to handle uncertainty in data\n",
        "- **Supervised Learning** - How to learn from labeled data\n",
        "- **Unsupervised Learning** - How to find patterns in unlabeled data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
