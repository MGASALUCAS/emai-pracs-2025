{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Actor-Critic: Combining Policy and Value Methods\n",
        "\n",
        "> **\"Actor-Critic methods combine the best of both policy and value approaches.\"**\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the actor-critic architecture and its advantages\n",
        "- Implement A2C (Advantage Actor-Critic) from scratch\n",
        "- Learn about advantage estimation and variance reduction\n",
        "- Master modern actor-critic algorithms (A3C, PPO, SAC)\n",
        "- Apply actor-critic methods to complex RL problems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Actor-Critic Methods\n",
        "\n",
        "### What are Actor-Critic Methods?\n",
        "Actor-Critic methods combine the benefits of both policy gradient methods (actor) and value function methods (critic). The actor learns the policy, while the critic learns the value function to provide better estimates for policy updates.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "#### 1. Actor (Policy)\n",
        "- Learns the policy Ï€(a|s)\n",
        "- Updated using policy gradient with value function estimates\n",
        "\n",
        "#### 2. Critic (Value Function)\n",
        "- Learns the state-value function V(s) or action-value function Q(s,a)\n",
        "- Provides better estimates for policy updates\n",
        "\n",
        "#### 3. Advantage Function\n",
        "A(s,a) = Q(s,a) - V(s) measures how much better an action is compared to the average.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actor-Critic agent class defined successfully!\n"
          ]
        }
      ],
      "source": [
        "class ActorCriticAgent:\n",
        "    \"\"\"Actor-Critic agent implementation.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_states, n_actions, learning_rate_actor=0.01, \n",
        "                 learning_rate_critic=0.01, gamma=0.99):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.learning_rate_actor = learning_rate_actor\n",
        "        self.learning_rate_critic = learning_rate_critic\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # Actor parameters (policy)\n",
        "        self.theta = np.random.randn(n_states, n_actions) * 0.1\n",
        "        \n",
        "        # Critic parameters (value function)\n",
        "        self.w = np.random.randn(n_states) * 0.1\n",
        "        \n",
        "    def softmax_policy(self, state):\n",
        "        \"\"\"Compute policy using softmax function.\"\"\"\n",
        "        logits = self.theta[state]\n",
        "        exp_logits = np.exp(logits - np.max(logits))\n",
        "        probabilities = exp_logits / np.sum(exp_logits)\n",
        "        return probabilities\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select action according to current policy.\"\"\"\n",
        "        probabilities = self.softmax_policy(state)\n",
        "        action = np.random.choice(self.n_actions, p=probabilities)\n",
        "        return action\n",
        "    \n",
        "    def value_function(self, state):\n",
        "        \"\"\"Compute state value function.\"\"\"\n",
        "        return self.w[state]\n",
        "    \n",
        "    def update_actor(self, state, action, advantage):\n",
        "        \"\"\"Update actor (policy) parameters.\"\"\"\n",
        "        probabilities = self.softmax_policy(state)\n",
        "        \n",
        "        # Policy gradient\n",
        "        policy_gradient = np.zeros(self.n_actions)\n",
        "        policy_gradient[action] = 1 - probabilities[action]\n",
        "        \n",
        "        # Update parameters\n",
        "        self.theta[state] += self.learning_rate_actor * advantage * policy_gradient\n",
        "    \n",
        "    def update_critic(self, state, target_value):\n",
        "        \"\"\"Update critic (value function) parameters.\"\"\"\n",
        "        current_value = self.value_function(state)\n",
        "        td_error = target_value - current_value\n",
        "        \n",
        "        # Update parameters\n",
        "        self.w[state] += self.learning_rate_critic * td_error\n",
        "\n",
        "print(\"Actor-Critic agent class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'GridWorldPG' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m episode_rewards, episode_lengths\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Create environment and agent\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mGridWorldPG\u001b[49m(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     55\u001b[0m agent \u001b[38;5;241m=\u001b[39m ActorCriticAgent(n_states\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mn_states, n_actions\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mn_actions, \n\u001b[0;32m     56\u001b[0m                         learning_rate_actor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, learning_rate_critic\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Actor-Critic agent...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'GridWorldPG' is not defined"
          ]
        }
      ],
      "source": [
        "# Train Actor-Critic agent\n",
        "def train_actor_critic(agent, env, episodes=1000):\n",
        "    \"\"\"Train Actor-Critic agent on the environment.\"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    \n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 100\n",
        "        total_reward = 0\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            # Select action\n",
        "            action = agent.select_action(state)\n",
        "            \n",
        "            # Take step\n",
        "            next_state, reward, done = env.step(action)\n",
        "            \n",
        "            # Compute target value\n",
        "            if done:\n",
        "                target_value = reward\n",
        "            else:\n",
        "                target_value = reward + agent.gamma * agent.value_function(next_state)\n",
        "            \n",
        "            # Compute advantage\n",
        "            current_value = agent.value_function(state)\n",
        "            advantage = target_value - current_value\n",
        "            \n",
        "            # Update critic\n",
        "            agent.update_critic(state, target_value)\n",
        "            \n",
        "            # Update actor\n",
        "            agent.update_actor(state, action, advantage)\n",
        "            \n",
        "            # Update state and tracking variables\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "        \n",
        "        # Store episode statistics\n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_lengths.append(steps)\n",
        "        \n",
        "        # Print progress\n",
        "        if episode % 100 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}\")\n",
        "    \n",
        "    return episode_rewards, episode_lengths\n",
        "\n",
        "# Create environment and agent\n",
        "env = GridWorldPG(size=4)\n",
        "agent = ActorCriticAgent(n_states=env.n_states, n_actions=env.n_actions, \n",
        "                        learning_rate_actor=0.01, learning_rate_critic=0.01, gamma=0.99)\n",
        "\n",
        "print(\"Training Actor-Critic agent...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Train the agent\n",
        "episode_rewards, episode_lengths = train_actor_critic(agent, env, episodes=1000)\n",
        "\n",
        "# Plot training progress\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot episode rewards\n",
        "axes[0].plot(episode_rewards, alpha=0.6)\n",
        "axes[0].set_xlabel('Episode')\n",
        "axes[0].set_ylabel('Total Reward')\n",
        "axes[0].set_title('Episode Rewards During Training')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot moving average\n",
        "window = 50\n",
        "if len(episode_rewards) >= window:\n",
        "    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
        "    axes[0].plot(range(window-1, len(episode_rewards)), moving_avg, 'r-', linewidth=2, label=f'Moving Average ({window})')\n",
        "    axes[0].legend()\n",
        "\n",
        "# Plot episode lengths\n",
        "axes[1].plot(episode_lengths, alpha=0.6)\n",
        "axes[1].set_xlabel('Episode')\n",
        "axes[1].set_ylabel('Episode Length')\n",
        "axes[1].set_title('Episode Lengths During Training')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}\")\n",
        "print(f\"Final average episode length: {np.mean(episode_lengths[-100:]):.2f}\")\n",
        "\n",
        "# Compare with REINFORCE\n",
        "print(\"\\nComparing Actor-Critic with REINFORCE:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Train REINFORCE agent for comparison\n",
        "reinforce_agent = REINFORCEAgent(n_states=env.n_states, n_actions=env.n_actions, \n",
        "                                learning_rate=0.01, gamma=0.99)\n",
        "reinforce_rewards, reinforce_lengths = train_reinforce(reinforce_agent, env, episodes=1000)\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(episode_rewards, alpha=0.6, label='Actor-Critic')\n",
        "plt.plot(reinforce_rewards, alpha=0.6, label='REINFORCE')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Reward Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(episode_lengths, alpha=0.6, label='Actor-Critic')\n",
        "plt.plot(reinforce_lengths, alpha=0.6, label='REINFORCE')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Episode Length')\n",
        "plt.title('Episode Length Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Actor-Critic final average reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
        "print(f\"REINFORCE final average reward: {np.mean(reinforce_rewards[-100:]):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
