{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q-Learning: From Theory to Implementation\n",
        "\n",
        "> **\"Q-Learning is the foundation of modern reinforcement learning.\"**\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the Q-Learning algorithm and its mathematical foundation\n",
        "- Implement Q-Learning from scratch for grid world environments\n",
        "- Learn about exploration strategies (ε-greedy, UCB)\n",
        "- Master hyperparameter tuning and convergence analysis\n",
        "- Apply Q-Learning to solve real-world problems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Q-Learning Algorithm\n",
        "\n",
        "### Q-Function\n",
        "Q(s,a) represents the expected cumulative reward for taking action a in state s.\n",
        "\n",
        "### Q-Learning Update Rule\n",
        "**Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]**\n",
        "\n",
        "Where:\n",
        "- α: learning rate\n",
        "- r: immediate reward\n",
        "- γ: discount factor\n",
        "- s': next state\n",
        "- a': next action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "    \"\"\"Q-Learning agent implementation from scratch.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_states, n_actions, learning_rate=0.1, discount_factor=0.9, \n",
        "                 epsilon=0.1, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        \n",
        "        # Initialize Q-table with zeros\n",
        "        self.q_table = np.zeros((n_states, n_actions))\n",
        "        \n",
        "    def choose_action(self, state, training=True):\n",
        "        \"\"\"Choose action using epsilon-greedy policy.\"\"\"\n",
        "        if training and np.random.random() < self.epsilon:\n",
        "            # Explore: choose random action\n",
        "            return np.random.choice(self.n_actions)\n",
        "        else:\n",
        "            # Exploit: choose best action\n",
        "            return np.argmax(self.q_table[state])\n",
        "    \n",
        "    def update_q_table(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Update Q-table using Q-learning update rule.\"\"\"\n",
        "        if done:\n",
        "            # Terminal state\n",
        "            target = reward\n",
        "        else:\n",
        "            # Non-terminal state\n",
        "            target = reward + self.discount_factor * np.max(self.q_table[next_state])\n",
        "        \n",
        "        # Q-learning update rule\n",
        "        self.q_table[state, action] += self.learning_rate * (target - self.q_table[state, action])\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decay exploration rate.\"\"\"\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "print(\"Q-Learning agent class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid World Environment\n",
        "class GridWorld:\n",
        "    \"\"\"Simple grid world environment for Q-learning.\"\"\"\n",
        "    \n",
        "    def __init__(self, size=5):\n",
        "        self.size = size\n",
        "        self.n_states = size * size\n",
        "        self.n_actions = 4  # Up, Down, Left, Right\n",
        "        \n",
        "        # Define grid\n",
        "        self.grid = np.zeros((size, size))\n",
        "        self.start_pos = (0, 0)\n",
        "        self.goal_pos = (size-1, size-1)\n",
        "        self.current_pos = self.start_pos\n",
        "        \n",
        "        # Actions: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        self.action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment to initial state.\"\"\"\n",
        "        self.current_pos = self.start_pos\n",
        "        return self._pos_to_state(self.current_pos)\n",
        "    \n",
        "    def _pos_to_state(self, pos):\n",
        "        \"\"\"Convert position to state index.\"\"\"\n",
        "        return pos[0] * self.size + pos[1]\n",
        "    \n",
        "    def _state_to_pos(self, state):\n",
        "        \"\"\"Convert state index to position.\"\"\"\n",
        "        return (state // self.size, state % self.size)\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Take a step in the environment.\"\"\"\n",
        "        # Calculate new position\n",
        "        new_row = self.current_pos[0] + self.action_effects[action][0]\n",
        "        new_col = self.current_pos[1] + self.action_effects[action][1]\n",
        "        \n",
        "        # Check boundaries\n",
        "        if 0 <= new_row < self.size and 0 <= new_col < self.size:\n",
        "            self.current_pos = (new_row, new_col)\n",
        "        \n",
        "        # Calculate reward\n",
        "        if self.current_pos == self.goal_pos:\n",
        "            reward = 100  # Goal reached\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -1   # Step penalty\n",
        "            done = False\n",
        "        \n",
        "        return self._pos_to_state(self.current_pos), reward, done\n",
        "    \n",
        "    def render(self):\n",
        "        \"\"\"Render the current state of the environment.\"\"\"\n",
        "        grid_display = self.grid.copy()\n",
        "        grid_display[self.current_pos] = 2  # Agent position\n",
        "        grid_display[self.goal_pos] = 3     # Goal position\n",
        "        \n",
        "        print(\"Grid World:\")\n",
        "        print(\"0: Empty, 2: Agent, 3: Goal\")\n",
        "        print(grid_display)\n",
        "        print(f\"Current position: {self.current_pos}\")\n",
        "        print(f\"Goal position: {self.goal_pos}\")\n",
        "\n",
        "print(\"Grid World environment defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Q-Learning agent\n",
        "def train_q_learning(agent, env, episodes=1000):\n",
        "    \"\"\"Train Q-Learning agent on the environment.\"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    \n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        max_steps = 100  # Prevent infinite episodes\n",
        "        \n",
        "        while steps < max_steps:\n",
        "            # Choose action\n",
        "            action = agent.choose_action(state, training=True)\n",
        "            \n",
        "            # Take step\n",
        "            next_state, reward, done = env.step(action)\n",
        "            \n",
        "            # Update Q-table\n",
        "            agent.update_q_table(state, action, reward, next_state, done)\n",
        "            \n",
        "            # Update state and tracking variables\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "            \n",
        "            # Check if episode is done\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Decay exploration rate\n",
        "        agent.decay_epsilon()\n",
        "        \n",
        "        # Store episode statistics\n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_lengths.append(steps)\n",
        "        \n",
        "        # Print progress\n",
        "        if episode % 100 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
        "    \n",
        "    return episode_rewards, episode_lengths\n",
        "\n",
        "# Create environment and agent\n",
        "env = GridWorld(size=5)\n",
        "agent = QLearningAgent(n_states=env.n_states, n_actions=env.n_actions, \n",
        "                      learning_rate=0.1, discount_factor=0.9, \n",
        "                      epsilon=0.9, epsilon_decay=0.995, epsilon_min=0.01)\n",
        "\n",
        "print(\"Training Q-Learning agent...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Train the agent\n",
        "episode_rewards, episode_lengths = train_q_learning(agent, env, episodes=1000)\n",
        "\n",
        "# Plot training progress\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot episode rewards\n",
        "ax1.plot(episode_rewards, alpha=0.6)\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Total Reward')\n",
        "ax1.set_title('Episode Rewards During Training')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot moving average\n",
        "window = 50\n",
        "if len(episode_rewards) >= window:\n",
        "    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
        "    ax1.plot(range(window-1, len(episode_rewards)), moving_avg, 'r-', linewidth=2, label=f'Moving Average ({window})')\n",
        "    ax1.legend()\n",
        "\n",
        "# Plot episode lengths\n",
        "ax2.plot(episode_lengths, alpha=0.6)\n",
        "ax2.set_xlabel('Episode')\n",
        "ax2.set_ylabel('Episode Length')\n",
        "ax2.set_title('Episode Lengths During Training')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}\")\n",
        "print(f\"Final average episode length: {np.mean(episode_lengths[-100:]):.2f}\")\n",
        "print(f\"Final epsilon: {agent.epsilon:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
