{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Policy Gradient: Direct Policy Optimization\n",
        "\n",
        "> **\"Policy gradient methods learn policies directly without value functions.\"**\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the policy gradient theorem and its mathematical foundation\n",
        "- Implement REINFORCE algorithm from scratch\n",
        "- Learn about advantage functions and variance reduction\n",
        "- Master policy gradient variants (A2C, PPO)\n",
        "- Apply policy gradient methods to continuous control problems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Policy Gradient Methods\n",
        "\n",
        "### What are Policy Gradient Methods?\n",
        "Policy gradient methods are a class of reinforcement learning algorithms that directly optimize the policy function without needing to learn a value function. They work by adjusting the policy parameters in the direction that increases the expected return.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "#### 1. Policy Function\n",
        "A policy π(a|s) defines the probability of taking action a in state s.\n",
        "\n",
        "#### 2. Policy Gradient Theorem\n",
        "The gradient of the expected return with respect to the policy parameters is:\n",
        "$\\nabla_\\theta J(\\theta) = \\mathbb{E}[\\nabla_\\theta \\log \\pi(a|s) Q(s,a)]$\n",
        "\n",
        "#### 3. REINFORCE Algorithm\n",
        "A simple policy gradient algorithm that uses the total return as an estimate of Q(s,a).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class REINFORCEAgent:\n",
        "    \"\"\"REINFORCE policy gradient agent implementation.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_states, n_actions, learning_rate=0.01, gamma=0.99):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # Initialize policy parameters (weights for each state-action pair)\n",
        "        self.theta = np.random.randn(n_states, n_actions) * 0.1\n",
        "        \n",
        "    def softmax_policy(self, state):\n",
        "        \"\"\"Compute policy using softmax function.\"\"\"\n",
        "        # Get logits for current state\n",
        "        logits = self.theta[state]\n",
        "        \n",
        "        # Apply softmax to get probabilities\n",
        "        exp_logits = np.exp(logits - np.max(logits))  # Subtract max for numerical stability\n",
        "        probabilities = exp_logits / np.sum(exp_logits)\n",
        "        \n",
        "        return probabilities\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select action according to current policy.\"\"\"\n",
        "        probabilities = self.softmax_policy(state)\n",
        "        action = np.random.choice(self.n_actions, p=probabilities)\n",
        "        return action\n",
        "    \n",
        "    def compute_returns(self, rewards, gamma):\n",
        "        \"\"\"Compute discounted returns for each timestep.\"\"\"\n",
        "        returns = []\n",
        "        G = 0\n",
        "        \n",
        "        # Compute returns backwards\n",
        "        for reward in reversed(rewards):\n",
        "            G = reward + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        \n",
        "        return returns\n",
        "    \n",
        "    def update_policy(self, states, actions, returns):\n",
        "        \"\"\"Update policy parameters using REINFORCE algorithm.\"\"\"\n",
        "        for state, action, G in zip(states, actions, returns):\n",
        "            # Get current policy probabilities\n",
        "            probabilities = self.softmax_policy(state)\n",
        "            \n",
        "            # Compute policy gradient\n",
        "            # ∇log π(a|s) = 1 - π(a|s) for the selected action\n",
        "            # ∇log π(a'|s) = -π(a'|s) for other actions\n",
        "            policy_gradient = np.zeros(self.n_actions)\n",
        "            policy_gradient[action] = 1 - probabilities[action]\n",
        "            \n",
        "            # Update parameters\n",
        "            self.theta[state] += self.learning_rate * G * policy_gradient\n",
        "\n",
        "print(\"REINFORCE agent class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Grid World Environment for Policy Gradient\n",
        "class GridWorldPG:\n",
        "    \"\"\"Grid world environment for policy gradient methods.\"\"\"\n",
        "    \n",
        "    def __init__(self, size=4):\n",
        "        self.size = size\n",
        "        self.n_states = size * size\n",
        "        self.n_actions = 4  # Up, Down, Left, Right\n",
        "        \n",
        "        # Define grid\n",
        "        self.grid = np.zeros((size, size))\n",
        "        self.start_pos = (0, 0)\n",
        "        self.goal_pos = (size-1, size-1)\n",
        "        self.current_pos = self.start_pos\n",
        "        \n",
        "        # Actions: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        self.action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment to initial state.\"\"\"\n",
        "        self.current_pos = self.start_pos\n",
        "        return self._pos_to_state(self.current_pos)\n",
        "    \n",
        "    def _pos_to_state(self, pos):\n",
        "        \"\"\"Convert position to state index.\"\"\"\n",
        "        return pos[0] * self.size + pos[1]\n",
        "    \n",
        "    def _state_to_pos(self, state):\n",
        "        \"\"\"Convert state index to position.\"\"\"\n",
        "        return (state // self.size, state % self.size)\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Take a step in the environment.\"\"\"\n",
        "        # Calculate new position\n",
        "        new_row = self.current_pos[0] + self.action_effects[action][0]\n",
        "        new_col = self.current_pos[1] + self.action_effects[action][1]\n",
        "        \n",
        "        # Check boundaries\n",
        "        if 0 <= new_row < self.size and 0 <= new_col < self.size:\n",
        "            self.current_pos = (new_row, new_col)\n",
        "        \n",
        "        # Calculate reward\n",
        "        if self.current_pos == self.goal_pos:\n",
        "            reward = 100  # Goal reached\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -1   # Step penalty\n",
        "            done = False\n",
        "        \n",
        "        return self._pos_to_state(self.current_pos), reward, done\n",
        "\n",
        "# Train REINFORCE agent\n",
        "def train_reinforce(agent, env, episodes=1000):\n",
        "    \"\"\"Train REINFORCE agent on the environment.\"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    \n",
        "    for episode in range(episodes):\n",
        "        # Generate episode\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        \n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 100\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            # Select action\n",
        "            action = agent.select_action(state)\n",
        "            \n",
        "            # Take step\n",
        "            next_state, reward, done = env.step(action)\n",
        "            \n",
        "            # Store experience\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            \n",
        "            # Update state\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "        \n",
        "        # Compute returns\n",
        "        returns = agent.compute_returns(rewards, agent.gamma)\n",
        "        \n",
        "        # Update policy\n",
        "        agent.update_policy(states, actions, returns)\n",
        "        \n",
        "        # Store episode statistics\n",
        "        episode_rewards.append(sum(rewards))\n",
        "        episode_lengths.append(steps)\n",
        "        \n",
        "        # Print progress\n",
        "        if episode % 100 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}\")\n",
        "    \n",
        "    return episode_rewards, episode_lengths\n",
        "\n",
        "# Create environment and agent\n",
        "env = GridWorldPG(size=4)\n",
        "agent = REINFORCEAgent(n_states=env.n_states, n_actions=env.n_actions, \n",
        "                       learning_rate=0.01, gamma=0.99)\n",
        "\n",
        "print(\"Training REINFORCE agent...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Train the agent\n",
        "episode_rewards, episode_lengths = train_reinforce(agent, env, episodes=1000)\n",
        "\n",
        "# Plot training progress\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot episode rewards\n",
        "axes[0].plot(episode_rewards, alpha=0.6)\n",
        "axes[0].set_xlabel('Episode')\n",
        "axes[0].set_ylabel('Total Reward')\n",
        "axes[0].set_title('Episode Rewards During Training')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot moving average\n",
        "window = 50\n",
        "if len(episode_rewards) >= window:\n",
        "    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
        "    axes[0].plot(range(window-1, len(episode_rewards)), moving_avg, 'r-', linewidth=2, label=f'Moving Average ({window})')\n",
        "    axes[0].legend()\n",
        "\n",
        "# Plot episode lengths\n",
        "axes[1].plot(episode_lengths, alpha=0.6)\n",
        "axes[1].set_xlabel('Episode')\n",
        "axes[1].set_ylabel('Episode Length')\n",
        "axes[1].set_title('Episode Lengths During Training')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}\")\n",
        "print(f\"Final average episode length: {np.mean(episode_lengths[-100:]):.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
