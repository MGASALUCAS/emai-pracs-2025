{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression: Classification Fundamentals\n",
        "\n",
        "> **\"Logistic regression is the foundation of classification in machine learning.\"**\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the mathematical foundation of logistic regression\n",
        "- Implement logistic regression from scratch using gradient descent\n",
        "- Learn about the sigmoid function and decision boundaries\n",
        "- Master multiclass classification strategies\n",
        "- Apply logistic regression to real-world classification problems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Mathematical Foundation\n",
        "\n",
        "### Logistic Function (Sigmoid)\n",
        "The logistic function transforms any real number into a probability between 0 and 1.\n",
        "\n",
        "**Mathematical definition:**\n",
        "$\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
        "\n",
        "Where $z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$\n",
        "\n",
        "### Cost Function\n",
        "Logistic regression uses the log-likelihood cost function:\n",
        "\n",
        "$J(\\beta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y_i \\log(h_\\beta(x_i)) + (1-y_i)\\log(1-h_\\beta(x_i))]$\n",
        "\n",
        "Where:\n",
        "- $h_\\beta(x_i)$ is the predicted probability\n",
        "- $y_i$ is the actual label (0 or 1)\n",
        "- $m$ is the number of samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the logistic function\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid function implementation.\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Generate z values\n",
        "z = np.linspace(-10, 10, 100)\n",
        "sigma_z = sigmoid(z)\n",
        "\n",
        "# Plot sigmoid function\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(z, sigma_z, 'b-', linewidth=2)\n",
        "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Decision threshold (0.5)')\n",
        "plt.axvline(x=0, color='g', linestyle='--', alpha=0.7, label='z = 0')\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('σ(z)')\n",
        "plt.title('Logistic (Sigmoid) Function')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Show decision boundary\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(z, sigma_z, 'b-', linewidth=2, label='σ(z)')\n",
        "plt.fill_between(z, 0, 0.5, alpha=0.3, color='red', label='Class 0 (y=0)')\n",
        "plt.fill_between(z, 0.5, 1, alpha=0.3, color='green', label='Class 1 (y=1)')\n",
        "plt.axhline(y=0.5, color='black', linestyle='-', linewidth=2, label='Decision boundary')\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('σ(z)')\n",
        "plt.title('Decision Boundary')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key properties of the sigmoid function:\")\n",
        "print(f\"σ(0) = {sigmoid(0):.3f}\")\n",
        "print(f\"σ(∞) = {sigmoid(10):.3f} (approaches 1)\")\n",
        "print(f\"σ(-∞) = {sigmoid(-10):.3f} (approaches 0)\")\n",
        "print(f\"σ(2) = {sigmoid(2):.3f}\")\n",
        "print(f\"σ(-2) = {sigmoid(-2):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample data for logistic regression\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
        "                          n_informative=2, n_clusters_per_class=1, \n",
        "                          random_state=42)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
        "                                                    random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Dataset Overview:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
        "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
        "print(f\"Class distribution in training set: {np.bincount(y_train)}\")\n",
        "print(f\"Class distribution in test set: {np.bincount(y_test)}\")\n",
        "\n",
        "# Visualize the data\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Original data\n",
        "plt.subplot(1, 3, 1)\n",
        "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Original Data')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "# Training data\n",
        "plt.subplot(1, 3, 2)\n",
        "scatter = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', alpha=0.7)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Training Data')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "# Test data\n",
        "plt.subplot(1, 3, 3)\n",
        "scatter = plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', alpha=0.7)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Test Data')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train logistic regression model\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "print(f\"AUC Score: {auc_score:.3f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"True Negatives: {cm[0,0]}, False Positives: {cm[0,1]}\")\n",
        "print(f\"False Negatives: {cm[1,0]}, True Positives: {cm[1,1]}\")\n",
        "\n",
        "# Visualize results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Confusion Matrix Heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
        "axes[0,0].set_title('Confusion Matrix')\n",
        "axes[0,0].set_xlabel('Predicted')\n",
        "axes[0,0].set_ylabel('Actual')\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "axes[0,1].plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
        "axes[0,1].plot([0, 1], [0, 1], 'r--', alpha=0.7, label='Random Classifier')\n",
        "axes[0,1].set_xlabel('False Positive Rate')\n",
        "axes[0,1].set_ylabel('True Positive Rate')\n",
        "axes[0,1].set_title('ROC Curve')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Decision Boundary\n",
        "h = 0.02\n",
        "x_min, x_max = X_test_scaled[:, 0].min() - 1, X_test_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_test_scaled[:, 1].min() - 1, X_test_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "Z = log_reg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "axes[1,0].contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "scatter = axes[1,0].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], c=y_test, cmap='viridis')\n",
        "axes[1,0].set_xlabel('Feature 1 (scaled)')\n",
        "axes[1,0].set_ylabel('Feature 2 (scaled)')\n",
        "axes[1,0].set_title('Decision Boundary')\n",
        "\n",
        "# Probability Distribution\n",
        "axes[1,1].hist(y_pred_proba[y_test == 0], bins=20, alpha=0.7, label='Class 0', color='red')\n",
        "axes[1,1].hist(y_pred_proba[y_test == 1], bins=20, alpha=0.7, label='Class 1', color='blue')\n",
        "axes[1,1].axvline(x=0.5, color='black', linestyle='--', label='Decision threshold')\n",
        "axes[1,1].set_xlabel('Predicted Probability')\n",
        "axes[1,1].set_ylabel('Frequency')\n",
        "axes[1,1].set_title('Probability Distribution')\n",
        "axes[1,1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
