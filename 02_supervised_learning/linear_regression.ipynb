{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression: From Theory to Implementation\n",
        "\n",
        "> **\"Linear regression is the foundation of machine learning - simple yet powerful.\"**\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the mathematical foundation of linear regression\n",
        "- Implement linear regression from scratch using NumPy\n",
        "- Learn about regularization techniques (Ridge, Lasso)\n",
        "- Master model evaluation and validation techniques\n",
        "- Apply linear regression to real-world problems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Mathematical Foundation\n",
        "\n",
        "### Simple Linear Regression\n",
        "**Model**: y = β₀ + β₁x + ε\n",
        "\n",
        "Where:\n",
        "- y: dependent variable (target)\n",
        "- x: independent variable (feature)\n",
        "- β₀: intercept (bias term)\n",
        "- β₁: slope (coefficient)\n",
        "- ε: error term (residuals)\n",
        "\n",
        "### Multiple Linear Regression\n",
        "**Model**: y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε\n",
        "\n",
        "### Matrix Form\n",
        "**y = Xβ + ε**\n",
        "\n",
        "Where:\n",
        "- y: (n×1) target vector\n",
        "- X: (n×p) feature matrix\n",
        "- β: (p×1) coefficient vector\n",
        "- ε: (n×1) error vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import pandas as pd\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Implementation from Scratch\n",
        "\n",
        "### Normal Equation\n",
        "**β = (XᵀX)⁻¹Xᵀy**\n",
        "\n",
        "This gives us the optimal coefficients that minimize the sum of squared residuals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearRegressionFromScratch:\n",
        "    \"\"\"Linear Regression implementation from scratch using normal equation.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.coefficients = None\n",
        "        self.intercept = None\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the linear regression model.\n",
        "        \n",
        "        Parameters:\n",
        "        X: Feature matrix (n_samples, n_features)\n",
        "        y: Target vector (n_samples,)\n",
        "        \"\"\"\n",
        "        # Add bias term (intercept) to X\n",
        "        X_with_bias = np.column_stack([np.ones(X.shape[0]), X])\n",
        "        \n",
        "        # Calculate coefficients using normal equation\n",
        "        # β = (X^T X)^(-1) X^T y\n",
        "        XtX = np.dot(X_with_bias.T, X_with_bias)\n",
        "        XtX_inv = np.linalg.inv(XtX)\n",
        "        Xty = np.dot(X_with_bias.T, y)\n",
        "        \n",
        "        coefficients = np.dot(XtX_inv, Xty)\n",
        "        \n",
        "        # Separate intercept and coefficients\n",
        "        self.intercept = coefficients[0]\n",
        "        self.coefficients = coefficients[1:]\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the fitted model.\n",
        "        \n",
        "        Parameters:\n",
        "        X: Feature matrix (n_samples, n_features)\n",
        "        \n",
        "        Returns:\n",
        "        y_pred: Predicted values (n_samples,)\n",
        "        \"\"\"\n",
        "        if self.coefficients is None:\n",
        "            raise ValueError(\"Model must be fitted before making predictions\")\n",
        "        \n",
        "        return self.intercept + np.dot(X, self.coefficients)\n",
        "    \n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Calculate R-squared score.\n",
        "        \n",
        "        Parameters:\n",
        "        X: Feature matrix (n_samples, n_features)\n",
        "        y: True target values (n_samples,)\n",
        "        \n",
        "        Returns:\n",
        "        r2: R-squared score\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X)\n",
        "        ss_res = np.sum((y - y_pred) ** 2)\n",
        "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
        "        r2 = 1 - (ss_res / ss_tot)\n",
        "        return r2\n",
        "\n",
        "# Test our implementation\n",
        "print(\"Linear Regression class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test our implementation\n",
        "print(\"Testing Linear Regression Implementation:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(100, 2)  # 100 samples, 2 features\n",
        "true_coeffs = np.array([2.5, -1.3])\n",
        "true_intercept = 3.2\n",
        "y = true_intercept + X @ true_coeffs + np.random.normal(0, 0.5, 100)\n",
        "\n",
        "print(f\"True coefficients: {true_coeffs}\")\n",
        "print(f\"True intercept: {true_intercept}\")\n",
        "\n",
        "# Fit our model\n",
        "model = LinearRegressionFromScratch()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(f\"\\nEstimated coefficients: {model.coefficients}\")\n",
        "print(f\"Estimated intercept: {model.intercept:.3f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "r2_score = model.score(X, y)\n",
        "\n",
        "print(f\"\\nR-squared score: {r2_score:.3f}\")\n",
        "\n",
        "# Compare with sklearn\n",
        "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
        "sklearn_model = SklearnLinearRegression()\n",
        "sklearn_model.fit(X, y)\n",
        "sklearn_r2 = sklearn_model.score(X, y)\n",
        "\n",
        "print(f\"Sklearn R-squared: {sklearn_r2:.3f}\")\n",
        "print(f\"Difference: {abs(r2_score - sklearn_r2):.6f}\")\n",
        "\n",
        "# Visualize results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Scatter plot of predictions vs actual\n",
        "ax1.scatter(y, y_pred, alpha=0.6)\n",
        "ax1.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
        "ax1.set_xlabel('Actual Values')\n",
        "ax1.set_ylabel('Predicted Values')\n",
        "ax1.set_title('Predictions vs Actual Values')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals plot\n",
        "residuals = y - y_pred\n",
        "ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "ax2.axhline(y=0, color='r', linestyle='--')\n",
        "ax2.set_xlabel('Predicted Values')\n",
        "ax2.set_ylabel('Residuals')\n",
        "ax2.set_title('Residuals Plot')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Regularization Techniques\n",
        "\n",
        "### Ridge Regression (L2 Regularization)\n",
        "Adds a penalty term proportional to the sum of squared coefficients.\n",
        "\n",
        "**Objective Function:**\n",
        "$J(\\beta) = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\alpha\\sum_{j=1}^{p}\\beta_j^2$\n",
        "\n",
        "### Lasso Regression (L1 Regularization)\n",
        "Adds a penalty term proportional to the sum of absolute coefficients.\n",
        "\n",
        "**Objective Function:**\n",
        "$J(\\beta) = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\alpha\\sum_{j=1}^{p}|\\beta_j|$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different regularization techniques\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate data with multicollinearity\n",
        "np.random.seed(42)\n",
        "n_samples, n_features = 100, 10\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Create multicollinear features\n",
        "X[:, 2] = X[:, 0] + 0.1 * np.random.randn(n_samples)\n",
        "X[:, 3] = X[:, 1] + 0.1 * np.random.randn(n_samples)\n",
        "\n",
        "# True coefficients (only first 3 are non-zero)\n",
        "true_coeffs = np.zeros(n_features)\n",
        "true_coeffs[:3] = [2.5, -1.3, 0.8]\n",
        "true_intercept = 3.2\n",
        "y = true_intercept + X @ true_coeffs + np.random.normal(0, 0.5, n_samples)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit different models\n",
        "models = {\n",
        "    'Linear Regression': LinearRegressionFromScratch(),\n",
        "    'Ridge (α=0.1)': Ridge(alpha=0.1),\n",
        "    'Ridge (α=1.0)': Ridge(alpha=1.0),\n",
        "    'Lasso (α=0.1)': Lasso(alpha=0.1),\n",
        "    'Lasso (α=1.0)': Lasso(alpha=1.0)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    if name == 'Linear Regression':\n",
        "        model.fit(X_scaled, y)\n",
        "        coeffs = np.concatenate([[model.intercept], model.coefficients])\n",
        "        r2 = model.score(X_scaled, y)\n",
        "    else:\n",
        "        model.fit(X_scaled, y)\n",
        "        coeffs = np.concatenate([[model.intercept_], model.coef_])\n",
        "        r2 = model.score(X_scaled, y)\n",
        "    \n",
        "    results[name] = {\n",
        "        'coefficients': coeffs,\n",
        "        'r2_score': r2\n",
        "    }\n",
        "\n",
        "# Visualize coefficient comparison\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "feature_names = ['Intercept'] + [f'Feature {i}' for i in range(n_features)]\n",
        "x_pos = np.arange(len(feature_names))\n",
        "\n",
        "for i, (name, result) in enumerate(results.items()):\n",
        "    if i < 6:  # Only plot first 6 models\n",
        "        ax = axes[i]\n",
        "        coeffs = result['coefficients']\n",
        "        r2 = result['r2_score']\n",
        "        \n",
        "        bars = ax.bar(x_pos, coeffs, alpha=0.7)\n",
        "        ax.set_title(f'{name}\\nR² = {r2:.3f}')\n",
        "        ax.set_xlabel('Features')\n",
        "        ax.set_ylabel('Coefficient Value')\n",
        "        ax.set_xticks(x_pos)\n",
        "        ax.set_xticklabels(feature_names, rotation=45)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Highlight true non-zero coefficients\n",
        "        for j in range(1, 4):  # First 3 features are non-zero\n",
        "            bars[j].set_color('red')\n",
        "            bars[j].set_alpha(0.8)\n",
        "\n",
        "# Remove empty subplot\n",
        "axes[5].remove()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print comparison table\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Model':<20} {'R² Score':<10} {'L1 Norm':<10} {'L2 Norm':<10}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for name, result in results.items():\n",
        "    coeffs = result['coefficients'][1:]  # Exclude intercept\n",
        "    l1_norm = np.sum(np.abs(coeffs))\n",
        "    l2_norm = np.sqrt(np.sum(coeffs**2))\n",
        "    r2 = result['r2_score']\n",
        "    \n",
        "    print(f\"{name:<20} {r2:<10.3f} {l1_norm:<10.3f} {l2_norm:<10.3f}\")\n",
        "\n",
        "print(f\"\\nTrue coefficients: {true_coeffs}\")\n",
        "print(\"Note: L1 norm encourages sparsity (Lasso), L2 norm encourages small coefficients (Ridge)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
