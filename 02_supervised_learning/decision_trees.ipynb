{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decision Trees: Tree-Based Classification\n",
        "\n",
        "> **\"Decision trees make complex decisions simple and interpretable.\"**\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand how decision trees work and their mathematical foundation\n",
        "- Implement decision tree algorithm from scratch\n",
        "- Learn about splitting criteria (Gini, entropy, information gain)\n",
        "- Master tree pruning and overfitting prevention\n",
        "- Apply decision trees to classification and regression problems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree, export_text\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "import pandas as pd\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Decision Tree Fundamentals\n",
        "\n",
        "### What is a Decision Tree?\n",
        "A decision tree is a flowchart-like structure where each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label or decision.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "#### 1. Splitting Criteria\n",
        "- **Gini Impurity**: Measures the probability of misclassifying a randomly chosen element\n",
        "- **Entropy**: Measures the amount of information in a dataset\n",
        "- **Information Gain**: Reduction in entropy after a split\n",
        "\n",
        "#### 2. Stopping Criteria\n",
        "- Maximum depth\n",
        "- Minimum samples per leaf\n",
        "- Minimum samples to split\n",
        "- Maximum number of leaf nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Iris dataset for demonstration\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
        "                                                    random_state=42, stratify=y)\n",
        "\n",
        "print(\"Iris Dataset Overview:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Features: {feature_names}\")\n",
        "print(f\"Target classes: {target_names}\")\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "# Train decision tree\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nDecision Tree Performance:\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Visualize the decision tree\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(dt_classifier, feature_names=feature_names, class_names=target_names, \n",
        "          filled=True, rounded=True, fontsize=10)\n",
        "plt.title(\"Decision Tree Visualization\")\n",
        "plt.show()\n",
        "\n",
        "# Print tree rules\n",
        "print(\"\\nDecision Tree Rules:\")\n",
        "print(\"=\" * 50)\n",
        "tree_rules = export_text(dt_classifier, feature_names=feature_names)\n",
        "print(tree_rules)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different tree depths\n",
        "depths = [1, 2, 3, 4, 5, 10]\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for depth in depths:\n",
        "    # Train model\n",
        "    dt = DecisionTreeClassifier(random_state=42, max_depth=depth)\n",
        "    dt.fit(X_train, y_train)\n",
        "    \n",
        "    # Calculate accuracies\n",
        "    train_acc = accuracy_score(y_train, dt.predict(X_train))\n",
        "    test_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "    \n",
        "    train_accuracies.append(train_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "# Plot accuracy vs depth\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(depths, train_accuracies, 'o-', label='Training Accuracy', linewidth=2)\n",
        "plt.plot(depths, test_accuracies, 's-', label='Test Accuracy', linewidth=2)\n",
        "plt.xlabel('Tree Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Decision Tree Performance vs Depth')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"Accuracy by Tree Depth:\")\n",
        "print(\"=\" * 40)\n",
        "for i, depth in enumerate(depths):\n",
        "    print(f\"Depth {depth}: Train={train_accuracies[i]:.3f}, Test={test_accuracies[i]:.3f}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = dt_classifier.feature_importances_\n",
        "print(f\"\\nFeature Importance:\")\n",
        "print(\"=\" * 40)\n",
        "for i, (feature, importance) in enumerate(zip(feature_names, feature_importance)):\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(feature_names, feature_importance, alpha=0.7)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Feature Importance in Decision Tree')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, importance in zip(bars, feature_importance):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{importance:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
